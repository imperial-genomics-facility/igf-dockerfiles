{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illumina QC prediction run {{ RUN_ID }}\n",
    "* **Notebook version:** v0.0.1\n",
    "* **Created by:** NIHR Imperial BRC Genomics Facility\n",
    "* **Maintained by:** NIHR Imperial BRC Genomics Facility\n",
    "* **Docker image path:** [Dockerfile](https://github.com/imperial-genomics-facility/igf-dockerfiles/tree/main/illumina-interop/Dockerfile_v1)\n",
    "* **Notebook code path:** [Templates](https://github.com/imperial-genomics-facility/igf-dockerfiles/tree/main/illumina-interop/templates)\n",
    "* **Created on:** {{ DATE_TAG }}\n",
    "* **Contact us:** [NIHR Imperial BRC Genomics Facility - Contact us](https://www.imperial.ac.uk/medicine/research-and-impact/facilities/genomics-facility/contact-us/)\n",
    "* **License:** Apache [License 2.0](https://github.com/imperial-genomics-facility/igf-dockerfiles/blob/main/LICENSE)\n",
    "\n",
    "Send us your suggestions (or PRs) about how to improve this notebook.\n",
    "\n",
    "Please add the following statement in all publications if you use any part of this notebook for your analysis: _“The NIHR Imperial BRC Genomics Facility has provided resources and support that have contributed to the research results reported within this paper.”._\n",
    "\n",
    "## Input file used:\n",
    "  * Model path: {{ MODEL_PATH }}\n",
    "  * Tile data for run: {{ PARQUET_PATH }}\n",
    "  * Number of CPUs to use: {{ NUM_CPU }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import altair  as alt\n",
    "from IPython.display import HTML\n",
    "alt.renderers.enable(\"html\")\n",
    "\n",
    "file_name = \"{{ MODEL_PATH }}\"\n",
    "model = pickle.load(open(file_name, \"rb\"))\n",
    "\n",
    "## load parquet data\n",
    "conf = SparkConf()\n",
    "conf = \\\n",
    "    conf.\\\n",
    "    setMaster(\"local[{{ NUM_CPU }}]\").\\\n",
    "    setAppName(\"InterOpReport\").\\\n",
    "    set(\"spark.log.level\", \"OFF\").\\\n",
    "    set(\"spark.driver.extraJavaOptions\", \"-Dlog4j.logger.org=OFF\").\\\n",
    "    set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\").\\\n",
    "    set(\"spark.executor.memory\", \"{{ RAM_GB }}g\").\\\n",
    "    set(\"spark.executor.cores\", \"{{ NUM_CPU }}\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = \\\n",
    "    SparkSession(sc).\\\n",
    "    builder.\\\n",
    "    getOrCreate()\n",
    "pred_df = spark.read.parquet('{{ PARQUET_PATH }}')\n",
    "\n",
    "## convert to Pandas DF\n",
    "pred_pdf = pred_df.toPandas()\n",
    "## change column type\n",
    "pred_pdf = \\\n",
    "    pred_pdf.astype({\n",
    "        'PCT_ClusterCountPF': float,\n",
    "        'PCT_DensityPF': float,\n",
    "        'PCT_Occupied': float})\n",
    "## get subset of columns for prediction\n",
    "X_pred = pred_pdf[['PCT_ClusterCountPF', 'PCT_DensityPF',\n",
    "         'mean_CalledCount_A', 'mean_CalledCount_T', 'mean_CalledCount_G',\n",
    "         'mean_CalledCount_C', 'PCT_Q30', 'PCT_Occupied',\n",
    "         'intensity_c1', 'slope_p', 'offset_p', 'slope_pr', 'offset_pr']]\n",
    "## transform columns\n",
    "ct = ColumnTransformer(\n",
    "        [(\"scaling\",\n",
    "          StandardScaler(), \n",
    "          ['mean_CalledCount_A', \n",
    "           'mean_CalledCount_T', \n",
    "           'mean_CalledCount_G',\n",
    "           'mean_CalledCount_C'])],\n",
    "        remainder='passthrough')\n",
    "X_pred = ct.fit_transform(X_pred)\n",
    "## predict flowcell type status\n",
    "y_pred = model.predict(X_pred)\n",
    "## add labels back to Pandas DF for plotting\n",
    "pred_pdf['is_failed'] = y_pred\n",
    "## add extra axis for histogram plot\n",
    "def add_hist_axis_for_plotting(s)-> pd.Series:\n",
    "    tile = s['Tile']\n",
    "    s['h_x'] = int(str(int(tile))[0])\n",
    "    s['h_y'] = int(str(int(tile))[1:])\n",
    "    return s\n",
    "pred_pdf = \\\n",
    "    pred_pdf.apply(lambda s: add_hist_axis_for_plotting(s), axis=1)\n",
    "\n",
    "display(HTML(\"<h2>Prediction status of Illumina flowcell</h2>\"))\n",
    "for lane, l_data in pred_pdf.groupby('Lane'):\n",
    "    chart1 = \\\n",
    "            alt.Chart(l_data).mark_rect().encode(\n",
    "                x=alt.X('h_y:O', axis=alt.Axis(labels=False, ticks=False, title=None)),\n",
    "                y=alt.Y('h_x:O', axis=alt.Axis(labels=False, ticks=False, title=None)),\n",
    "                color=alt.Color('is_failed:O').scale(scheme='lightgreyred'),\n",
    "                tooltip=['Lane:N', 'Tile:O', 'is_failed:O']\n",
    "            ).configure_view(\n",
    "                step=200,\n",
    "                strokeWidth=5\n",
    "            ).configure_axis(\n",
    "                domain=False\n",
    "            ).properties(\n",
    "                title=f'Prediction of Lane {lane}',\n",
    "                width=1080,\n",
    "                height=100\n",
    "            )\n",
    "    display(chart1)\n",
    "\n",
    "## Get PCT tiles failed\n",
    "display(HTML(\"<h2>PCT tiles failed</h2>\"))\n",
    "print(f\"PCT tiles failed: {len(pred_pdf[pred_pdf['is_failed']==1])/len(pred_pdf['is_failed']):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
