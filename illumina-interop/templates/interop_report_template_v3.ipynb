{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illumina InterOp Report for run {{ RUN_ID }}\n",
    "* **Notebook version:** v0.0.3\n",
    "* **Created by:** NIHR Imperial BRC Genomics Facility\n",
    "* **Maintained by:** NIHR Imperial BRC Genomics Facility\n",
    "* **Docker image path:** [Dockerfile](https://github.com/imperial-genomics-facility/igf-dockerfiles/tree/main/illumina-interop/Dockerfile_v1)\n",
    "* **Notebook code path:** [Templates](https://github.com/imperial-genomics-facility/igf-dockerfiles/tree/main/illumina-interop/templates)\n",
    "* **Created on:** {{ DATE_TAG }}\n",
    "* **Contact us:** [NIHR Imperial BRC Genomics Facility - Contact us](https://www.imperial.ac.uk/medicine/research-and-impact/facilities/genomics-facility/contact-us/)\n",
    "* **License:** Apache [License 2.0](https://github.com/imperial-genomics-facility/igf-dockerfiles/blob/main/LICENSE)\n",
    "\n",
    "## Code source\n",
    "This notebook was developed using codes from the following sources:\n",
    "  * [Illumina InterOp](http://illumina.github.io/interop/index.html)\n",
    "\n",
    "Send us your suggestions (or PRs) about how to improve this notebook.\n",
    "\n",
    "Please add the following statement in all publications if you use any part of this notebook for your analysis: _“The NIHR Imperial BRC Genomics Facility has provided resources and support that have contributed to the research results reported within this paper.”._\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "  * [Run information](#Run-information)\n",
    "    * [Run overview](#Run-overview)\n",
    "    * [Run stats for each read groups](#Run-stats-for-each-read-groups)\n",
    "    * [Run stats for each lanes](#Run-stats-for-each-lanes)\n",
    "  * [Plots](#Plots)\n",
    "    * [Tile plots](#Tile-plots)\n",
    "    * [Intensity plots](#Intensity-plots)\n",
    "    * [Box plots for % ClusterCountPF and % DensityPF](#Box-plots-for---ClusterCountPF-and---DensityPF)\n",
    "    * [PCA plots](#PCA-plots)\n",
    "    * [Scatter plot for % Occupied vs % PF](#Scatter-plot-for---Occupied-vs---PF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('html')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')\n",
    "import re, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import (Tuple, Iterator)\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import (LongType, DoubleType)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.functions import col, lit\n",
    "from IPython.display import HTML\n",
    "from sklearn.decomposition import PCA\n",
    "import altair as alt\n",
    "alt.renderers.enable(\"html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "##\n",
    "## creating InterOp metrics\n",
    "##\n",
    "set -eo pipefail\n",
    "\n",
    "METRICS_DIR={{ METRICS_DIR }}\n",
    "RUN_DIR={{ RUN_DIR }}\n",
    "RUN_ID={{ RUN_ID }}\n",
    "\n",
    "mkdir -p ${METRICS_DIR}\n",
    "\n",
    "if [[ -e ${RUN_DIR}/InterOp/CorrectedIntMetricsOut.bin ]];then\n",
    "  dumptext ${RUN_DIR} --metric=CorrectedInt > ${METRICS_DIR}/${RUN_ID}_CorrectedInt.csv\n",
    "fi\n",
    "if [[ -e ${RUN_DIR}/InterOp/ExtractionMetricsOut.bin ]];then\n",
    "  dumptext ${RUN_DIR} --metric=Extraction > ${METRICS_DIR}/${RUN_ID}_Extraction.csv\n",
    "fi\n",
    "if [[ -e ${RUN_DIR}/InterOp/QMetricsOut.bin ]];then\n",
    "  dumptext ${RUN_DIR} --metric=Q > ${METRICS_DIR}/${RUN_ID}_Q.csv\n",
    "fi\n",
    "if [[ -e ${RUN_DIR}/InterOp/TileMetricsOut.bin ]];then\n",
    "  dumptext ${RUN_DIR} --metric=Tile > ${METRICS_DIR}/${RUN_ID}_Tile.csv\n",
    "fi\n",
    "if [[ -e ${RUN_DIR}/InterOp/ImageMetricsOut.bin ]];then\n",
    "  dumptext ${RUN_DIR} --metric=Image > ${METRICS_DIR}/${RUN_ID}_Image.csv\n",
    "fi\n",
    "if [[ -e ${RUN_DIR}/InterOp/QMetricsByLaneOut.bin ]];then\n",
    "  dumptext ${RUN_DIR} --metric=QByLane > ${METRICS_DIR}/${RUN_ID}_QByLane.csv\n",
    "fi\n",
    "if [[ -e ${RUN_DIR}/InterOp/QMetricsByLaneOut.bin ]];then\n",
    "  dumptext ${RUN_DIR} --metric=QCollapsed > ${METRICS_DIR}/${RUN_ID}_QCollapsed.csv\n",
    "fi\n",
    "if [[ -e ${RUN_DIR}/InterOp/EmpiricalPhasingMetricsOut.bin ]];then\n",
    "  dumptext ${RUN_DIR} --metric=EmpiricalPhasing > ${METRICS_DIR}/${RUN_ID}_EmpiricalPhasing.csv\n",
    "fi\n",
    "if [[ -e ${RUN_DIR}/InterOp/ExtendedTileMetricsOut.bin ]];then\n",
    "  dumptext ${RUN_DIR} --metric=ExtendedTile > ${METRICS_DIR}/${RUN_ID}_ExtendedTile.csv\n",
    "fi\n",
    "if [[ -e ${RUN_DIR}/InterOp/ErrorMetricsOut.bin ]];then\n",
    "  dumptext ${RUN_DIR} --metric=Error > ${METRICS_DIR}/${RUN_ID}_Error.csv\n",
    "fi\n",
    "if [[ -e ${RUN_DIR}/InterOp/SummaryRunMetricsOut.bin ]];then\n",
    "  dumptext ${RUN_DIR} --metric=SummaryRun > ${METRICS_DIR}/${RUN_ID}_SummaryRun.csv\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## collection interop metrics\n",
    "metrics_list = list()\n",
    "interop_metrics_path = \"{{ METRICS_DIR }}\"\n",
    "runinfo_xml = os.path.join(\"{{ RUN_DIR }}\", \"RunInfo.xml\")\n",
    "runparameters_xml = os.path.join(\"{{ RUN_DIR }}\", \"RunParameters.xml\")\n",
    "overview_csv_output = \"{{ OVERVIEW_CSV_OUTPUT }}\"\n",
    "tile_parquet_output = \"{{ TILE_PARQUET_OUTOUT }}\"\n",
    "\n",
    "for i in os.listdir(interop_metrics_path):\n",
    "    if i.endswith(\"_CorrectedInt.csv\"):\n",
    "        metrics_list.append(\n",
    "            {'CorrectedInt': os.path.join(interop_metrics_path, i)})\n",
    "    if i.endswith(\"_Error.csv\"):\n",
    "        metrics_list.append(\n",
    "            {'Error': os.path.join(interop_metrics_path, i)})\n",
    "    if i.endswith(\"_Extraction.csv\"):\n",
    "        metrics_list.append(\n",
    "            {'Extraction': os.path.join(interop_metrics_path, i)})\n",
    "    if i.endswith(\"_QByLane.csv\"):\n",
    "        metrics_list.append(\n",
    "            {'QByLane': os.path.join(interop_metrics_path, i)})\n",
    "    if i.endswith(\"_Q.csv\"):\n",
    "        metrics_list.append(\n",
    "            {'Q': os.path.join(interop_metrics_path, i)})\n",
    "    if i.endswith(\"_Tile.csv\"):\n",
    "        metrics_list.append(\n",
    "            {'Tile': os.path.join(interop_metrics_path, i)})\n",
    "    if i.endswith(\"_EmpiricalPhasing.csv\"):\n",
    "        metrics_list.append(\n",
    "            {'EmpiricalPhasing': os.path.join(interop_metrics_path, i)})\n",
    "    if i.endswith(\"_Image.csv\"):\n",
    "        metrics_list.append(\n",
    "            {'Image': os.path.join(interop_metrics_path, i)})\n",
    "    if i.endswith(\"_QCollapsed.csv\"):\n",
    "        metrics_list.append(\n",
    "            {'QCollapsed': os.path.join(interop_metrics_path, i)})\n",
    "    if i.endswith(\"_ExtendedTile.csv\"):\n",
    "        metrics_list.append(\n",
    "            {'ExtendedTile': os.path.join(interop_metrics_path, i)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf = \\\n",
    "    conf.\\\n",
    "    setMaster(\"local[{{ NUM_CPU }}]\").\\\n",
    "    setAppName(\"InterOpReport\").\\\n",
    "    set(\"spark.log.level\", \"OFF\").\\\n",
    "    set(\"spark.driver.extraJavaOptions\", \"-Dlog4j.logger.org=OFF\").\\\n",
    "    set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\").\\\n",
    "    set(\"spark.executor.memory\", \"{{ RAM_GB }}g\").\\\n",
    "    set(\"spark.executor.cores\", \"{{ NUM_CPU }}\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = \\\n",
    "    SparkSession(sc).\\\n",
    "    builder.\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_runinfo_xml(runInfoXml_path: str) -> Tuple[str, str, str, int, pd.DataFrame, int]:\n",
    "  \"\"\"\n",
    "  A function for reading RunInfo.xml file from Illumina sequencing run and returns data as Pandas DataFrame\n",
    "\n",
    "  :param runInfoXml_path: Filepath for RunInfo.xml\n",
    "  :returns: Run_id, Flowcell_id, Instrument_id, Lane count, a Pandas dataframe containing the run configuration data and total cycles\n",
    "  \"\"\"\n",
    "  try:\n",
    "    if not os.path.exists(runInfoXml_path):\n",
    "      raise IOError(\n",
    "        f'File {runInfoXml_path} not found')\n",
    "    run_id_pattern = \\\n",
    "      re.compile(\n",
    "        r'<Run Id=\\\"(\\S+)\\\"')\n",
    "    flowcell_pattern = \\\n",
    "      re.compile(\n",
    "        r'<Flowcell>(\\S+)\\</Flowcell>')\n",
    "    instrument_pattern = \\\n",
    "      re.compile(\n",
    "        r'<Instrument>(\\S+)</Instrument>')\n",
    "    read_pattern1 = \\\n",
    "      re.compile(\n",
    "        r'<Read Number=\\\"(\\d)\\\" NumCycles=\\\"(\\d+)\\\" IsIndexedRead=\\\"(Y|N)\\\"')\n",
    "    read_pattern2 = \\\n",
    "      re.compile(\n",
    "        r'<Read NumCycles=\\\"(\\d+)\\\" Number=\\\"(\\d)\\\" IsIndexedRead=\\\"(Y|N)\\\"')\n",
    "    lane_pattern = \\\n",
    "        re.compile(r'<FlowcellLayout LaneCount=\\\"(\\d)\\\"')\n",
    "    read_info = list()\n",
    "    with open(runInfoXml_path,'r') as fp:\n",
    "      for line in fp:\n",
    "        line = line.strip()\n",
    "        if line.startswith('<Read ') or \\\n",
    "           line.startswith('<Run Id') or \\\n",
    "           line.startswith('<Flowcell') or \\\n",
    "           line.startswith('<<FlowcellLayout') or \\\n",
    "           line.startswith('<Instrument'):\n",
    "          read_info.append(line)\n",
    "    run_id = ''\n",
    "    flowcell_id = ''\n",
    "    instrument_id = ''\n",
    "    lane_count = 0\n",
    "    reads_stat = list()\n",
    "    read_start = 0\n",
    "    total_cycles = 0\n",
    "    for i in read_info:\n",
    "      run_id_match = \\\n",
    "        re.match(run_id_pattern, i)\n",
    "      if run_id_match:\n",
    "        (run_id,) = \\\n",
    "          run_id_match.groups()\n",
    "      flowcell_id_match = \\\n",
    "        re.match(flowcell_pattern, i)\n",
    "      if flowcell_id_match:\n",
    "        (flowcell_id,) = \\\n",
    "          flowcell_id_match.groups()\n",
    "      instrument_id_match = \\\n",
    "        re.match(instrument_pattern, i)\n",
    "      if instrument_id_match:\n",
    "        (instrument_id,) = \\\n",
    "          instrument_id_match.groups()\n",
    "      lane_match = \\\n",
    "        re.match(lane_pattern, i)\n",
    "      if lane_match:\n",
    "        (lane_count,) = \\\n",
    "          lane_match.groups()\n",
    "      read_match1 = \\\n",
    "        re.match(read_pattern1, i)\n",
    "      if read_match1:\n",
    "        read_number, numcycle, index_read = \\\n",
    "          read_match1.groups()\n",
    "      else:\n",
    "        read_match2 = \\\n",
    "          re.match(read_pattern2, i)\n",
    "      if read_match2:\n",
    "        numcycle, read_number, index_read = \\\n",
    "          read_match2.groups()\n",
    "      if read_match1 or \\\n",
    "         read_match2:\n",
    "        reads_stat.append({\n",
    "          'read_id': int(read_number),\n",
    "          'cycles': int(numcycle),\n",
    "          'start_cycle': int(read_start)+1,\n",
    "          'finish_cycle': int(read_start)+int(numcycle),\n",
    "          'index_read': index_read})\n",
    "        read_start += int(numcycle)\n",
    "        total_cycles += int(numcycle)\n",
    "    reads_stat = pd.DataFrame(reads_stat)\n",
    "    if 'read_id' in reads_stat.columns:\n",
    "      reads_stat['read_id'] = \\\n",
    "        reads_stat['read_id'].astype(int)\n",
    "    return run_id, flowcell_id, instrument_id, int(lane_count), reads_stat, total_cycles\n",
    "  except Exception as e:\n",
    "    raise ValueError(\n",
    "      f'Failed to read RunInfo.xml for sequencing run, error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_runparameters_xml(runparametersXml_path: str) -> str:\n",
    "    \"\"\"\n",
    "    A function for reading RunParameters.xml file from Illumina sequencing run and returns flowcell info\n",
    "\n",
    "    :param runparametersXml_path: Filepath for RunParameters.xml\n",
    "    :returns: FlowCellMode\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(runparametersXml_path):\n",
    "            raise IOError(\n",
    "                f'File {runparametersXml_path} not found')\n",
    "        flowcell_mode_pattern = \\\n",
    "            re.compile(\n",
    "                r'\\s+?<FlowCellMode>(.*)</FlowCellMode>')\n",
    "        flowcell_mode = ''\n",
    "        with open(runparametersXml_path, 'r') as fp:\n",
    "            for line in fp:\n",
    "                if re.match(flowcell_mode_pattern, line):\n",
    "                    flowcell_mode = \\\n",
    "                        re.match(flowcell_mode_pattern, line).group(1)\n",
    "        return flowcell_mode\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            f'Failed to read RunParameters.xml for sequencing run, error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interop_data_frames(metrics_list: list[dict]) -> dict:\n",
    "    try:\n",
    "        df_dict = dict()\n",
    "        for entry in metrics_list:\n",
    "            for metrics_name, metrics_path in entry.items():\n",
    "                ## read text file\n",
    "                text_data = sc.textFile(metrics_path)\n",
    "                ## get headers\n",
    "                headers = list()\n",
    "                if metrics_name == 'Q' or \\\n",
    "                   metrics_name == 'QByLane':\n",
    "                    headers = \\\n",
    "                        text_data.\\\n",
    "                        filter(lambda line: not line.startswith('#')).\\\n",
    "                        map(lambda line: line.split(\",\")).\\\n",
    "                        filter(lambda lines: len(lines) > 3 ).\\\n",
    "                        first()\n",
    "                else:\n",
    "                    headers = \\\n",
    "                        text_data.\\\n",
    "                        filter(lambda line: not line.startswith('#')).\\\n",
    "                        map(lambda line: line.split(\",\")).\\\n",
    "                        first()\n",
    "                ## check if headers are present\n",
    "                if len(headers) == 0:\n",
    "                    raise ValueError(f\"No headers found for {metrics_name}\")\n",
    "                headers = [c.replace('(', '_').replace(')', '').replace(' ', '_') for c in headers]\n",
    "                header_schema = list()\n",
    "                header_schema = [f\"{c} STRING\" for c in headers]\n",
    "                schema = ','.join(header_schema)\n",
    "                ## read_data as rdd\n",
    "                rdd = \\\n",
    "                    text_data.\\\n",
    "                    filter(lambda line: not line.startswith('#')).\\\n",
    "                    filter(lambda line: not line.startswith('Lane')).\\\n",
    "                    map(lambda line: line.split(\",\")).\\\n",
    "                    filter(lambda lines: len(lines) == len(headers))\n",
    "                ## convert rdd to df\n",
    "                df = \\\n",
    "                    spark.createDataFrame(rdd, schema=schema)\n",
    "                df_dict.update({metrics_name: df})\n",
    "        return df_dict\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_read_cycles(reads_stat: pd.DataFrame) -> list:\n",
    "    try:\n",
    "        read_list = list()\n",
    "        for row in reads_stat.to_dict(orient='records'):\n",
    "            read_id = row.get('read_id')\n",
    "            index_read = row.get('index_read')\n",
    "            start_cycle = row.get('start_cycle')\n",
    "            finish_cycle = row.get('finish_cycle')\n",
    "            if index_read == 'N':\n",
    "                read_list.append([start_cycle, finish_cycle])\n",
    "        return read_list\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_stat = \\\n",
    "    pd.DataFrame([\n",
    "        {'read_id': 1, 'cycles': 28, 'start_cycle': 1, 'finish_cycle': 28, 'index_read': 'N'},\n",
    "        {'read_id': 2, 'cycles': 10, 'start_cycle': 29, 'finish_cycle': 38, 'index_read': 'Y'},\n",
    "        {'read_id': 3, 'cycles': 10, 'start_cycle': 39, 'finish_cycle': 48, 'index_read': 'Y'},\n",
    "        {'read_id': 4, 'cycles': 90, 'start_cycle': 49, 'finish_cycle': 138, 'index_read': 'N'}])\n",
    "read_cycles = \\\n",
    "    get_read_cycles(reads_stat=reads_stat)\n",
    "assert len(read_cycles) == 2\n",
    "assert len(read_cycles[0]) == 2\n",
    "assert read_cycles[0][0] == 1\n",
    "assert read_cycles[0][1] == 28\n",
    "assert read_cycles[1][0] == 49\n",
    "assert read_cycles[1][1] == 138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_callout_intensities_per_tile(\n",
    "        correctedIntDF: DataFrame,\n",
    "        reads_stat: pd.DataFrame) -> DataFrame:\n",
    "    try:\n",
    "        read_list = \\\n",
    "            get_read_cycles(reads_stat=reads_stat)\n",
    "        ## fill na\n",
    "        correctedIntDF = correctedIntDF.na.fill('0')\n",
    "        correctedIntDF = correctedIntDF.na.replace('nan', '0')\n",
    "        if len(read_list) == 2:\n",
    "            correctedIntTileDF = \\\n",
    "                correctedIntDF.\\\n",
    "                selectExpr(\n",
    "                    \"cast(Lane as int)\",\n",
    "                    \"cast(Tile as int)\",\n",
    "                    \"cast(Cycle as int)\",\n",
    "                    \"cast(CalledCount_A as long)\",\n",
    "                    \"cast(CalledCount_C as long)\",\n",
    "                    \"cast(CalledCount_G as long)\",\n",
    "                    \"cast(CalledCount_T as long)\").\\\n",
    "                where(\n",
    "                    ((col(\"Cycle\") >= read_list[0][0])&(col(\"Cycle\") < read_list[0][1]))|\\\n",
    "                    ((col(\"Cycle\") >= read_list[1][0])&(col(\"Cycle\") < read_list[1][1]))).\\\n",
    "                groupBy(\"Lane\", \"Tile\").\\\n",
    "                agg(\n",
    "                    sf.mean(col(\"CalledCount_A\")).alias(\"mean_CalledCount_A\"),\n",
    "                    sf.mean(col(\"CalledCount_T\")).alias(\"mean_CalledCount_T\"),\n",
    "                    sf.mean(col(\"CalledCount_G\")).alias(\"mean_CalledCount_G\"),\n",
    "                    sf.mean(col(\"CalledCount_C\")).alias(\"mean_CalledCount_C\"))\n",
    "        elif len(read_list) == 1:\n",
    "            correctedIntTileDF = \\\n",
    "                correctedIntDF.\\\n",
    "                selectExpr(\n",
    "                    \"cast(Lane as int)\",\n",
    "                    \"cast(Tile as int)\",\n",
    "                    \"cast(Cycle as int)\",\n",
    "                    \"cast(CalledCount_A as long)\",\n",
    "                    \"cast(CalledCount_C as long)\",\n",
    "                    \"cast(CalledCount_G as long)\",\n",
    "                    \"cast(CalledCount_T as long)\").\\\n",
    "                where(\n",
    "                    ((col(\"Cycle\") >= read_list[0][0])&(col(\"Cycle\") < read_list[0][1]))).\\\n",
    "                groupBy(\"Lane\", \"Tile\").\\\n",
    "                agg(\n",
    "                    sf.mean(col(\"CalledCount_A\")).alias(\"mean_CalledCount_A\"),\n",
    "                    sf.mean(col(\"CalledCount_T\")).alias(\"mean_CalledCount_T\"),\n",
    "                    sf.mean(col(\"CalledCount_G\")).alias(\"mean_CalledCount_G\"),\n",
    "                    sf.mean(col(\"CalledCount_C\")).alias(\"mean_CalledCount_C\"))\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Read cycles are unknown: {len(read_list)}\")\n",
    "        return correctedIntTileDF\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '1', 'CalledCount_A': '1981', 'CalledCount_T': '2434', 'CalledCount_G': '1987', 'CalledCount_C': '1945'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '2', 'CalledCount_A': '2050', 'CalledCount_T': '2275', 'CalledCount_G': '1990', 'CalledCount_C': '2032'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '3', 'CalledCount_A': '2026', 'CalledCount_T': '2159', 'CalledCount_G': '2014', 'CalledCount_C': '2148'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '4', 'CalledCount_A': '2106', 'CalledCount_T': '2009', 'CalledCount_G': '2030', 'CalledCount_C': '2202'}]))\n",
    "t = get_mean_callout_intensities_per_tile(t_df, reads_stat)\n",
    "t_pdf = t.toPandas()\n",
    "assert len(t_pdf.index) == 1\n",
    "assert t_pdf['mean_CalledCount_A'].values[0] == 2040.75\n",
    "assert t_pdf['mean_CalledCount_T'].values[0] == 2219.25\n",
    "assert t_pdf['mean_CalledCount_G'].values[0] == 2005.25\n",
    "assert t_pdf['mean_CalledCount_C'].values[0] == 2081.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_error_rate_per_tile(\n",
    "        errorDF: DataFrame,\n",
    "        reads_stat: pd.DataFrame) -> DataFrame:\n",
    "    try:\n",
    "        read_list = \\\n",
    "            get_read_cycles(reads_stat=reads_stat)\n",
    "        ## fill na\n",
    "        errorDF = errorDF.na.fill('0')\n",
    "        errorDF = errorDF.na.replace('nan', '0')\n",
    "        if len(read_list) == 2:\n",
    "            errorTileDF = \\\n",
    "                errorDF.\\\n",
    "                selectExpr(\n",
    "                    \"cast(Lane as int)\",\n",
    "                    \"cast(Tile as int)\",\n",
    "                    \"cast(Cycle as int)\",\n",
    "                    \"cast(ErrorRate as float)\").\\\n",
    "                    where(\n",
    "                        ((col(\"Cycle\") >= read_list[0][0])&(col(\"Cycle\") < read_list[0][1]))|\\\n",
    "                        ((col(\"Cycle\") >= read_list[1][0])&(col(\"Cycle\") < read_list[1][1]))).\\\n",
    "                    groupBy(\"Lane\", \"Tile\").\\\n",
    "                    agg(\n",
    "                        sf.mean(col(\"ErrorRate\")).alias(\"mean_ErrorRate\")).\\\n",
    "                    orderBy(\"Lane\", \"Tile\")\n",
    "        elif len(read_list) == 1:\n",
    "            errorTileDF = \\\n",
    "                errorDF.\\\n",
    "                selectExpr(\n",
    "                    \"cast(Lane as int)\",\n",
    "                    \"cast(Tile as int)\",\n",
    "                    \"cast(Cycle as int)\",\n",
    "                    \"cast(ErrorRate as float)\").\\\n",
    "                    where(((col(\"Cycle\") >= read_list[0][0])&(col(\"Cycle\") < read_list[0][1]))).\\\n",
    "                    groupBy(\"Lane\", \"Tile\").\\\n",
    "                    agg(\n",
    "                        sf.mean(col(\"ErrorRate\")).alias(\"mean_ErrorRate\")).\\\n",
    "                    orderBy(\"Lane\", \"Tile\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Read cycles are unknown: {len(read_list)}\")\n",
    "        return errorTileDF\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': 1, 'Tile': 2103, 'Cycle': 1, 'ErrorRate': 0.17},\n",
    "        {'Lane': 1, 'Tile': 2103, 'Cycle': 2, 'ErrorRate': 0.05},\n",
    "        {'Lane': 1, 'Tile': 2103, 'Cycle': 3, 'ErrorRate': 0.02},\n",
    "        {'Lane': 1, 'Tile': 2103, 'Cycle': 4, 'ErrorRate': 0.05}]))\n",
    "t = \\\n",
    "    get_mean_error_rate_per_tile(\n",
    "        errorDF=t_df,\n",
    "        reads_stat=reads_stat)\n",
    "t_pdf = t.toPandas()\n",
    "assert 'mean_ErrorRate' in t_pdf.columns\n",
    "assert f\"{float(t_pdf['mean_ErrorRate'].values[0]):.3f}\" == f\"{float(np.mean([0.17, 0.05, 0.02, 0.05])):.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_and_density_per_tile(\n",
    "        tileDF: DataFrame) -> DataFrame:\n",
    "    try:\n",
    "        ## fill na\n",
    "        tileDF = tileDF.na.fill('0')\n",
    "        tileDF = tileDF.na.replace('nan', '0')\n",
    "        ## get pct cluster and density PF\n",
    "        tileTileDF = \\\n",
    "            tileDF.\\\n",
    "            selectExpr(\n",
    "                \"cast(Lane as int)\",\n",
    "                \"cast(Tile as int)\",\n",
    "                \"cast(Read as int)\",\n",
    "                \"cast(ClusterCount as decimal)\",\n",
    "                \"cast(ClusterCountPF as decimal)\",\n",
    "                \"cast(Density as decimal)\",\n",
    "                \"cast(DensityPF as decimal)\").\\\n",
    "            groupBy(\"Lane\", \"Tile\").\\\n",
    "            agg(\n",
    "                (sf.sum(col(\"ClusterCountPF\"))/sf.sum(col(\"ClusterCount\"))).alias(\"PCT_ClusterCountPF\"),\n",
    "                (sf.sum(col(\"DensityPF\"))/sf.sum(col(\"Density\"))).alias(\"PCT_DensityPF\")).\\\n",
    "            orderBy(\"Lane\", \"Tile\")\n",
    "        return tileTileDF\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': 1, 'Tile': 1101, 'Read': 1, 'ClusterCount': 4091900, 'ClusterCountPF': 3430010, 'Density': 2961260, 'DensityPF': 2482260},\n",
    "        {'Lane': 1, 'Tile': 1101, 'Read': 4, 'ClusterCount': 4091900, 'ClusterCountPF': 3430010, 'Density': 2961260, 'DensityPF': 2482260}]))\n",
    "t = get_cluster_and_density_per_tile(\n",
    "        tileDF=t_df)\n",
    "t_pdf = t.toPandas()\n",
    "assert 'PCT_ClusterCountPF' in t_pdf.columns\n",
    "assert 'PCT_DensityPF' in t_pdf.columns\n",
    "assert float(t_pdf['PCT_ClusterCountPF'].values[0]) == 0.838243847601358782\n",
    "assert float(t_pdf['PCT_DensityPF'].values[0]) == 0.8382445310442177\n",
    "# print(t_pdf.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pct_q30_score_per_tile(\n",
    "        qDF: DataFrame,\n",
    "        reads_stat: pd.DataFrame) -> DataFrame:\n",
    "    try:\n",
    "        read_list = \\\n",
    "            get_read_cycles(reads_stat=reads_stat)\n",
    "        ## set q30 bins\n",
    "        qbin_count = \\\n",
    "            len([c for c in qDF.columns if c.startswith('Bin_')])\n",
    "        if qbin_count > 3:\n",
    "            qbin_list = [f\"Bin_{c}\" for c in range(30, 51)]\n",
    "        else:\n",
    "            qbin_list = [\"Bin_3\",]\n",
    "        non_qbin_list = [\n",
    "            c for c in qDF.columns\n",
    "                if c.startswith('Bin_') and c not in qbin_list ]\n",
    "        ## fill na\n",
    "        qDF = qDF.na.fill('0')\n",
    "        qDF = qDF.na.replace('nan', '0')\n",
    "        if len(read_list) == 2:\n",
    "            qTileDf = \\\n",
    "                qDF.\\\n",
    "                selectExpr(\n",
    "                    *(f\"cast({c} as long) as {c}\" for c in qDF.columns)).\\\n",
    "                filter(col(\"Tile\") > 0).\\\n",
    "                where(\n",
    "                    ((col(\"Cycle\") >= read_list[0][0])&(col(\"Cycle\") < read_list[0][1]))|\\\n",
    "                    ((col(\"Cycle\") >= read_list[1][0])&(col(\"Cycle\") < read_list[1][1]))).\\\n",
    "                groupBy(\"Lane\", \"Tile\").\\\n",
    "                agg(\n",
    "                    *(sf.sum(col(c)).alias(c)\n",
    "                        for c in qDF.columns \n",
    "                            if c.startswith('Bin_')),).\\\n",
    "                withColumn('non_q_bin_total', sf.expr(\"+\".join(non_qbin_list))).\\\n",
    "                withColumn('q_bin_total', sf.expr(\"+\".join(qbin_list))).\\\n",
    "                select(\n",
    "                    col(\"Lane\"),\n",
    "                    col(\"Tile\"),\n",
    "                    (col(\"q_bin_total\")/(col(\"non_q_bin_total\")+col(\"q_bin_total\"))).alias(\"PCT_Q30\")).\\\n",
    "                orderBy(\"Lane\", \"Tile\")\n",
    "        elif len(read_list) == 1:\n",
    "            qTileDf = \\\n",
    "                qDF.\\\n",
    "                selectExpr(\n",
    "                    *(f\"cast({c} as long) as {c}\" for c in qDF.columns)).\\\n",
    "                filter(col(\"Tile\") > 0).\\\n",
    "                where(((col(\"Cycle\") >= read_list[0][0])&(col(\"Cycle\") < read_list[0][1]))).\\\n",
    "                groupBy(\"Lane\", \"Tile\").\\\n",
    "                agg(\n",
    "                    *(sf.sum(col(c)).alias(c) \n",
    "                        for c in qDF.columns \n",
    "                            if c.startswith('Bin_')),).\\\n",
    "                withColumn('non_q_bin_total', sf.expr(\"+\".join(non_qbin_list))).\\\n",
    "                withColumn('q_bin_total', sf.expr(\"+\".join(qbin_list))).\\\n",
    "                select(\n",
    "                    col(\"Lane\"),\n",
    "                    col(\"Tile\"),\n",
    "                    (col(\"q_bin_total\")/(col(\"non_q_bin_total\")+col(\"q_bin_total\"))).alias(\"PCT_Q30\")).\\\n",
    "                orderBy(\"Lane\", \"Tile\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Read cycles are unknown: {len(read_list)}\")\n",
    "        return qTileDf\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '1', 'Bin_1': '55649', 'Bin_2': '84702', 'Bin_3': '3275160'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '2', 'Bin_1': '55649', 'Bin_2': '96568', 'Bin_3': '3263293'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '3', 'Bin_1': '30280', 'Bin_2': '65061', 'Bin_3': '3320171'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '4', 'Bin_1': '31916', 'Bin_2': '58105', 'Bin_3': '3325490'}]))\n",
    "t = get_pct_q30_score_per_tile(\n",
    "        qDF=t_df,\n",
    "        reads_stat=reads_stat)\n",
    "t_pdf = t.toPandas()\n",
    "assert 'PCT_Q30' in t_pdf.columns\n",
    "assert float(t_pdf['PCT_Q30'].values[0]) == 0.9650176796385666\n",
    "# t_pdf.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occupied_pct_per_tile(\n",
    "        tileDF: DataFrame,\n",
    "        extTileDF: DataFrame) -> DataFrame:\n",
    "    try:\n",
    "        ## fill na\n",
    "        tileDF = tileDF.na.fill('0')\n",
    "        tileDF = tileDF.na.replace('nan', '0')\n",
    "        ## fill na\n",
    "        extTileDF = extTileDF.na.fill('0')\n",
    "        extTileDF = extTileDF.na.replace('nan', '0')\n",
    "        ## calculate total cluster count per tile\n",
    "        tileTileDF = \\\n",
    "            tileDF.\\\n",
    "            selectExpr(\n",
    "                \"cast(Lane as int)\",\n",
    "                \"cast(Tile as int)\",\n",
    "                \"cast(Read as int)\",\n",
    "                \"cast(ClusterCount as decimal)\",\n",
    "                \"cast(ClusterCountPF as decimal)\").\\\n",
    "            filter(col(\"Read\")==1).\\\n",
    "            groupBy(\"Lane\", \"Tile\").\\\n",
    "            agg(\n",
    "                sf.sum(col(\"ClusterCount\")).alias(\"total_ClusterCount\"),\n",
    "                sf.sum(col(\"ClusterCountPF\")).alias(\"total_ClusterCountPF\")).\\\n",
    "            orderBy(\"Lane\", \"Tile\")\n",
    "        ## join extendedTile and Tile\n",
    "        joinExpression = \\\n",
    "            (tileTileDF['Lane'] == extTileDF['Lane'])&\\\n",
    "            (tileTileDF['Tile'] == extTileDF['Tile'])\n",
    "        joinType = \"inner\"\n",
    "        ## set alias\n",
    "        tileTileDF_alias = tileTileDF.alias('t')\n",
    "        extTileDF_alias = extTileDF.alias('et')\n",
    "        ## join DFs\n",
    "        joined_tiles = \\\n",
    "            tileTileDF_alias.\\\n",
    "            join(extTileDF_alias, joinExpression, joinType).\\\n",
    "            select(\n",
    "                \"t.Lane\",\n",
    "                \"t.Tile\",\n",
    "                \"t.total_ClusterCount\",\n",
    "                \"t.total_ClusterCountPF\",\n",
    "                \"et.OccupiedCount\")\n",
    "        ## calculate pct occupied\n",
    "        tiles_pct_occupied = \\\n",
    "            joined_tiles.\\\n",
    "            selectExpr(\n",
    "                \"Lane\", \"Tile\",\n",
    "                \"cast(OccupiedCount as decimal) / total_ClusterCount as PCT_Occupied\",\n",
    "                \"total_ClusterCountPF / total_ClusterCount as PCT_ClusterCountPF\")\n",
    "        return tiles_pct_occupied\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tileDF = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': 1, 'Tile': 1101, 'Read': 1, 'ClusterCount': 4091900, 'ClusterCountPF': 3430010},\n",
    "        {'Lane': 1, 'Tile': 1101, 'Read': 4, 'ClusterCount': 4091900, 'ClusterCountPF': 3430010}]))\n",
    "t_etileDF = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': 1, 'Tile': 1101, 'OccupiedCount': 3615010}]))\n",
    "t = get_occupied_pct_per_tile(t_tileDF, t_etileDF)\n",
    "t_pdf = t.toPandas()\n",
    "assert 'PCT_Occupied' in t_pdf.columns\n",
    "assert float(t_pdf['PCT_Occupied'].values[0]) == 3615010/4091900\n",
    "assert 'PCT_ClusterCountPF' in t_pdf.columns\n",
    "assert float(t_pdf['PCT_ClusterCountPF'].values[0]) == 3430010/4091900\n",
    "# t_pdf.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_phasing(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    for pdf in iterator:\n",
    "      cycle = pdf.Cycle\n",
    "      phasing = pdf.Phasing\n",
    "      prephasing = pdf.Prephasing\n",
    "      slope_p, offset_p = np.polyfit(cycle.values[0], np.float32(phasing.values[0]), 1)\n",
    "      slope_pr, offset_pr = np.polyfit(cycle.values[0], np.float32(prephasing.values[0]), 1)\n",
    "      pdf = pdf.assign(slope_p=slope_p)\n",
    "      pdf = pdf.assign(offset_p=offset_p)\n",
    "      pdf = pdf.assign(slope_pr=slope_pr)\n",
    "      pdf = pdf.assign(offset_pr=offset_pr)\n",
    "      yield pdf\n",
    "\n",
    "def get_phase_data_per_tile(\n",
    "        phaseDF: DataFrame,\n",
    "        reads_stat: pd.DataFrame) -> DataFrame:\n",
    "    try:\n",
    "        read_list = \\\n",
    "            get_read_cycles(reads_stat=reads_stat)\n",
    "        ## fill na\n",
    "        phaseDF = phaseDF.na.fill('0')\n",
    "        phaseDF = phaseDF.na.replace('nan', '0')\n",
    "        if len(read_list) == 2:\n",
    "            phaseTileDF = \\\n",
    "                phaseDF.\\\n",
    "                selectExpr(\n",
    "                    \"cast(Lane as int)\",\n",
    "                    \"cast(Tile as int)\",\n",
    "                    \"cast(Cycle as int)\",\n",
    "                    \"cast(Phasing as double)\",\n",
    "                    \"cast(Prephasing as double)\").\\\n",
    "                where(\n",
    "                    ((col(\"Cycle\") >= read_list[0][0])&(col(\"Cycle\") < read_list[0][1]))|\\\n",
    "                    ((col(\"Cycle\") >= read_list[1][0])&(col(\"Cycle\") < read_list[1][1]))).\\\n",
    "                withColumn(\"NewCycle\",\n",
    "                    sf.when(col(\"Cycle\") >= read_list[1][0], col(\"Cycle\") - (read_list[1][0] -1)).\\\n",
    "                            when(col(\"Cycle\") < read_list[1][0], col(\"Cycle\"))).\\\n",
    "                groupBy(\"Lane\", \"Tile\", \"NewCycle\").\\\n",
    "                agg(\n",
    "                    sf.mean(\"Phasing\").alias(\"Phasing\"),\n",
    "                    sf.mean(\"Prephasing\").alias(\"Prephasing\")\n",
    "                ).\\\n",
    "                groupBy(\"Lane\", \"Tile\").\\\n",
    "                agg(\n",
    "                    sf.collect_list(\"NewCycle\").alias(\"Cycle\"),\n",
    "                    sf.collect_list(\"Phasing\").alias(\"Phasing\"),\n",
    "                    sf.collect_list(\"Prephasing\").alias(\"Prephasing\"))\n",
    "        elif len(read_list) == 1:\n",
    "            phaseTileDF = \\\n",
    "                phaseDF.\\\n",
    "                selectExpr(\n",
    "                    \"cast(Lane as int)\",\n",
    "                    \"cast(Tile as int)\",\n",
    "                    \"cast(Cycle as int)\",\n",
    "                    \"cast(Phasing as double)\",\n",
    "                    \"cast(Prephasing as double)\").\\\n",
    "                filter((col(\"Cycle\") >= read_list[0][0])&(col(\"Cycle\") < read_list[0][1])).\\\n",
    "                groupBy(\"Lane\", \"Tile\").\\\n",
    "                agg(\n",
    "                    sf.collect_list(\"Cycle\").alias(\"Cycle\"),\n",
    "                    sf.collect_list(\"Phasing\").alias(\"Phasing\"),\n",
    "                    sf.collect_list(\"Prephasing\").alias(\"Prephasing\"))\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Read cycles are unknown: {len(read_list)}\")\n",
    "        ## get new df schema\n",
    "        phaseTileDF_schema = \\\n",
    "            phaseTileDF.\\\n",
    "            withColumn(\"slope_p\", lit(0.0).cast(DoubleType())).\\\n",
    "            withColumn(\"offset_p\", lit(0.0).cast(DoubleType())).\\\n",
    "            withColumn(\"slope_pr\", lit(0.0).cast(DoubleType())).\\\n",
    "            withColumn(\"offset_pr\", lit(0.0).cast(DoubleType())).\\\n",
    "            schema\n",
    "        ## calculate phasing data using Pandas \n",
    "        phaseDF_calc = \\\n",
    "            phaseTileDF.\\\n",
    "            mapInPandas(count_phasing, schema=phaseTileDF_schema)\n",
    "        return phaseDF_calc\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = \\\n",
    "    spark.createDataFrame(\n",
    "        pd.DataFrame([{'Lane': 1, 'Tile': 1101, 'Cycle': 1, 'Phasing': 3.0, 'Prephasing': 1.0},\n",
    "                      {'Lane': 1, 'Tile': 1101, 'Cycle': 2, 'Phasing': 1.25, 'Prephasing': 0.75},\n",
    "                      {'Lane': 1, 'Tile': 1101, 'Cycle': 3, 'Phasing': 1.0, 'Prephasing': 0.75},\n",
    "                      {'Lane': 1, 'Tile': 1101, 'Cycle': 4, 'Phasing': 1.75, 'Prephasing': 1.5},\n",
    "                      {'Lane': 1, 'Tile': 1102, 'Cycle': 1, 'Phasing': 3.1, 'Prephasing': 1.0},\n",
    "                      {'Lane': 1, 'Tile': 1102, 'Cycle': 2, 'Phasing': 1.5, 'Prephasing': 0.75},\n",
    "                      {'Lane': 1, 'Tile': 1102, 'Cycle': 3, 'Phasing': 1.5, 'Prephasing': 0.75},\n",
    "                      {'Lane': 1, 'Tile': 1102, 'Cycle': 4, 'Phasing': 1.7, 'Prephasing': 1.5}]))\n",
    "t = get_phase_data_per_tile(\n",
    "        phaseDF=t_df,\n",
    "        reads_stat=reads_stat)\n",
    "s_p, i_p = np.polyfit([1, 2, 3, 4], np.float32([3.0999999046325684, 1.5, 1.5, 1.7000000476837158]), 1)\n",
    "s_pr, i_pr = np.polyfit([1,2,3,4], np.float32([1.0, 0.75, 0.75, 1.5]), 1)\n",
    "t_pdf = t.toPandas()\n",
    "# print(s_p, i_p)\n",
    "# t.show(truncate=False)\n",
    "t_pdf = t_pdf[t_pdf['Tile'].astype(int)==1101]\n",
    "# print(t_pdf.to_dict(orient='records'))\n",
    "assert 'slope_p' in t_pdf.columns\n",
    "assert 'slope_pr' in t_pdf.columns\n",
    "assert 'offset_p' in t_pdf.columns\n",
    "assert 'offset_pr' in t_pdf.columns\n",
    "# print(f\"{float(t_pdf['offset_p'].values[0]):.1f} , {i_p:.1f}\")\n",
    "assert f\"{float(t_pdf['slope_p'].values[0]):.1f}\" == f\"{s_p:.1f}\"\n",
    "assert f\"{float(t_pdf['offset_p'].values[0]):.1f}\" == f\"{i_p:.1f}\"\n",
    "assert f\"{float(t_pdf['slope_pr'].values[0]):.1f}\" == f\"{s_pr:.1f}\"\n",
    "assert f\"{float(t_pdf['offset_pr'].values[0]):.1f}\" == f\"{i_pr:.1f}\"\n",
    "t_pdf = t.toPandas()\n",
    "t_pdf = t_pdf[t_pdf['Tile'].astype(int)==1102]\n",
    "s_p, i_p = np.polyfit([1,2,3,4], [3.1, 1.5, 1.5, 1.7], 1)\n",
    "assert f\"{float(t_pdf['slope_p'].values[0]):.1f}\" == f\"{s_p:.1f}\"\n",
    "assert f\"{float(t_pdf['offset_p'].values[0]):.1f}\" == f\"{i_p:.1f}\"\n",
    "# print(t_pdf.to_dict(orient='records'))\n",
    "t_df = \\\n",
    "    spark.createDataFrame(\n",
    "        pd.DataFrame([{'Lane': 1, 'Tile': 1101, 'Cycle': 1, 'Phasing': 3, 'Prephasing': 1},\n",
    "                      {'Lane': 1, 'Tile': 1101, 'Cycle': 2, 'Phasing': 1.25, 'Prephasing': 0.75},\n",
    "                      {'Lane': 1, 'Tile': 1101, 'Cycle': 3, 'Phasing': 1, 'Prephasing': 0.75},\n",
    "                      {'Lane': 1, 'Tile': 1101, 'Cycle': 4, 'Phasing': 1.75, 'Prephasing': 1.5},\n",
    "                      {'Lane': 1, 'Tile': 1101, 'Cycle': 49, 'Phasing': 2, 'Prephasing': 2},\n",
    "                      {'Lane': 1, 'Tile': 1101, 'Cycle': 50, 'Phasing': 1, 'Prephasing': 1},\n",
    "                      {'Lane': 1, 'Tile': 1101, 'Cycle': 51, 'Phasing': 1.25, 'Prephasing': 1.75},\n",
    "                      {'Lane': 1, 'Tile': 1101, 'Cycle': 52, 'Phasing': 2, 'Prephasing': 2}]))\n",
    "s_p, i_p = \\\n",
    "    np.polyfit(\n",
    "        [1,2,3,4], \n",
    "        [np.mean([3.0, 2.0]), np.mean([1.25, 1.0]), np.mean([1.0, 1.25]), np.mean([1.75, 2.0])], 1)\n",
    "t = get_phase_data_per_tile(\n",
    "        phaseDF=t_df,\n",
    "        reads_stat=reads_stat)\n",
    "t_pdf = t.toPandas()\n",
    "assert 'slope_p' in t_pdf.columns\n",
    "assert 'slope_pr' in t_pdf.columns\n",
    "assert 'offset_p' in t_pdf.columns\n",
    "assert 'offset_pr' in t_pdf.columns\n",
    "# print(s_p, i_p)\n",
    "# print(t_pdf.to_dict(orient=\"records\"))\n",
    "assert f\"{float(t_pdf['slope_p'].values[0]):.1f}\" == f\"{s_p:.1f}\"\n",
    "assert f\"{float(t_pdf['offset_p'].values[0]):.1f}\" == f\"{i_p:.1f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intensity_c1_per_tile(\n",
    "        extractionDF: DataFrame,\n",
    "        reads_stat: pd.DataFrame) -> DataFrame:\n",
    "    try:\n",
    "        read_list = \\\n",
    "            get_read_cycles(reads_stat=reads_stat)\n",
    "        ## fill na\n",
    "        extractionDF = extractionDF.na.fill('0')\n",
    "        extractionDF = extractionDF.na.replace('nan', '0')\n",
    "        ## get intensity col name\n",
    "        intensity_col = \"MaxIntensity_A\"\n",
    "        if \"MaxIntensity_blue\" in extractionDF.columns:\n",
    "            intensity_col = \"MaxIntensity_blue\"\n",
    "        if \"MaxIntensity_RED\" in extractionDF.columns:\n",
    "            intensity_col = \"MaxIntensity_RED\"\n",
    "        if len(read_list) == 2:\n",
    "            extractionTileDF = \\\n",
    "                extractionDF.\\\n",
    "                selectExpr(\n",
    "                    \"cast(Lane as int)\",\n",
    "                    \"cast(Tile as int)\",\n",
    "                    \"cast(Cycle as int)\",\n",
    "                    f\"cast({intensity_col} as long)\").\\\n",
    "                    filter(col(\"Tile\") > 0).\\\n",
    "                where(\n",
    "                    ((col(\"Cycle\") >= read_list[0][0])&(col(\"Cycle\") < read_list[0][1]))|\\\n",
    "                    ((col(\"Cycle\") >= read_list[1][0])&(col(\"Cycle\") < read_list[1][1]))).\\\n",
    "                groupBy(\"Lane\", \"Tile\").\\\n",
    "                agg(\n",
    "                    sf.mean(col(intensity_col)).alias(\"intensity_c1\")).\\\n",
    "                orderBy(\"Lane\", \"Tile\")\n",
    "        elif len(read_list) == 1:\n",
    "            extractionTileDF = \\\n",
    "                extractionDF.\\\n",
    "                selectExpr(\n",
    "                    \"cast(Lane as int)\",\n",
    "                    \"cast(Tile as int)\",\n",
    "                    \"cast(Cycle as int)\",\n",
    "                    f\"cast({intensity_col} as long)\").\\\n",
    "                    filter(col(\"Tile\") > 0).\\\n",
    "                where(((col(\"Cycle\") >= read_list[0][0])&(col(\"Cycle\") < read_list[0][1]))).\\\n",
    "                groupBy(\"Lane\", \"Tile\").\\\n",
    "                agg(\n",
    "                    sf.mean(col(intensity_col)).alias(\"intensity_c1\")).\\\n",
    "                orderBy(\"Lane\", \"Tile\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Read cycles are unknown: {len(read_list)}\")\n",
    "        return extractionTileDF\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '1', 'MaxIntensity_RED': '2264'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '2', 'MaxIntensity_RED': '2217'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '3', 'MaxIntensity_RED': '2295'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '4', 'MaxIntensity_RED': '2211'}]))\n",
    "t = \\\n",
    "    get_intensity_c1_per_tile(\n",
    "        extractionDF=t_df,\n",
    "        reads_stat=reads_stat)\n",
    "t_pdf = t.toPandas()\n",
    "assert 'intensity_c1' in t_pdf.columns\n",
    "assert float(t_pdf['intensity_c1'].values[0]) == np.mean([2264, 2217, 2295, 2211])\n",
    "# t_pdf.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_tile_data(\n",
    "        lane_count: int,\n",
    "        intensityC1TileDF: DataFrame,\n",
    "        phaseCalcDF: DataFrame,\n",
    "        tileOccupiedDF: DataFrame,\n",
    "        q30PctDF: DataFrame,\n",
    "        tileClusterAndDensityDF: DataFrame,\n",
    "        errorRateTileDF: DataFrame,\n",
    "        meanCalledintensityDF: DataFrame\n",
    "        ) -> DataFrame:\n",
    "    try:\n",
    "        ## intensity c1\n",
    "        intensityC1TileDF = \\\n",
    "            intensityC1TileDF.\\\n",
    "            repartition(lane_count, col(\"Lane\"))\n",
    "        intensityC1TileDF.\\\n",
    "            createOrReplaceTempView(\"intensityC1\")\n",
    "        ## phase\n",
    "        phaseCalcDF = \\\n",
    "            phaseCalcDF.\\\n",
    "            repartition(lane_count, col(\"Lane\"))\n",
    "        phaseCalcDF.\\\n",
    "            createOrReplaceTempView(\"phasing\")\n",
    "        ## occupied\n",
    "        tileOccupiedDF = \\\n",
    "            tileOccupiedDF.\\\n",
    "            repartition(lane_count, col(\"Lane\"))\n",
    "        tileOccupiedDF.\\\n",
    "            createOrReplaceTempView(\"tile_occupied\")\n",
    "        ## q30pct\n",
    "        q30PctDF = \\\n",
    "            q30PctDF.\\\n",
    "            repartition(lane_count, col(\"Lane\"))\n",
    "        q30PctDF.\\\n",
    "            createOrReplaceTempView(\"qual\")\n",
    "        ## cluster and density\n",
    "        tileClusterAndDensityDF = \\\n",
    "            tileClusterAndDensityDF.\\\n",
    "            repartition(lane_count, col(\"Lane\"))\n",
    "        tileClusterAndDensityDF.\\\n",
    "            createOrReplaceTempView(\"tile\")\n",
    "        ## error rate\n",
    "        errorRateTileDF = \\\n",
    "            errorRateTileDF.\\\n",
    "            repartition(lane_count, col(\"Lane\"))\n",
    "        errorRateTileDF.\\\n",
    "            createOrReplaceTempView(\"error\")\n",
    "        ## mean called intensity\n",
    "        meanCalledintensityDF = \\\n",
    "            meanCalledintensityDF.\\\n",
    "            repartition(lane_count, col(\"Lane\"))\n",
    "        meanCalledintensityDF.\\\n",
    "            createOrReplaceTempView(\"cint\")\n",
    "        ## join all\n",
    "        mergedDF = \\\n",
    "            spark.sql(\"\"\"\n",
    "                SELECT\n",
    "                tile.Lane,\n",
    "                tile.Tile,\n",
    "                tile.PCT_ClusterCountPF,\n",
    "                tile.PCT_DensityPF,\n",
    "                cint.mean_CalledCount_A,\n",
    "                cint.mean_CalledCount_T,\n",
    "                cint.mean_CalledCount_G,\n",
    "                cint.mean_CalledCount_C,\n",
    "                qual.PCT_Q30,\n",
    "                error.mean_ErrorRate,\n",
    "                tile_occupied.PCT_Occupied,\n",
    "                intensityC1.intensity_c1,\n",
    "                phasing.slope_p,\n",
    "                phasing.offset_p,\n",
    "                phasing.slope_pr,\n",
    "                phasing.offset_pr\n",
    "                FROM\n",
    "                tile\n",
    "                JOIN cint ON tile.Lane=cint.Lane AND tile.Tile=cint.Tile\n",
    "                JOIN qual ON tile.Lane=qual.Lane AND tile.Tile=qual.Tile\n",
    "                JOIN phasing ON tile.Lane=phasing.Lane AND tile.Tile=phasing.Tile\n",
    "                JOIN intensityC1 ON tile.Lane=intensityC1.Lane AND tile.Tile=intensityC1.Tile\n",
    "                LEFT JOIN tile_occupied ON tile.Lane=tile_occupied.Lane AND tile.Tile=tile_occupied.Tile\n",
    "                LEFT JOIN error ON tile.Lane=error.Lane AND tile.Tile=error.Tile\n",
    "                \"\"\")\n",
    "        ## fill na\n",
    "        mergedDF = mergedDF.na.fill(0.0)\n",
    "        return mergedDF\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_interop_metrics_and_merge_per_tile(df_dict: list, runinfo_xml: str) -> DataFrame:\n",
    "    try:\n",
    "#         df_dict = get_interop_data_frames(metrics_list)\n",
    "        run_id, flowcell_id, instrument_id, lane_count, reads_stat, _ = \\\n",
    "            read_runinfo_xml(runinfo_xml)\n",
    "        intensityC1TileDF = \\\n",
    "            get_intensity_c1_per_tile(\n",
    "                extractionDF=df_dict.get(\"Extraction\"),\n",
    "                reads_stat=reads_stat)\n",
    "        phaseCalcDF = \\\n",
    "            get_phase_data_per_tile(\n",
    "                phaseDF=df_dict.get(\"EmpiricalPhasing\"),\n",
    "                reads_stat=reads_stat)\n",
    "        if 'ExtendedTile' in df_dict:\n",
    "            tileOccupiedDF = \\\n",
    "                get_occupied_pct_per_tile(\n",
    "                    tileDF=df_dict.get(\"Tile\"),\n",
    "                    extTileDF=df_dict.get(\"ExtendedTile\"))\n",
    "        else:\n",
    "            tileOccupiedDF = \\\n",
    "                spark.createDataFrame(\n",
    "                    pd.DataFrame([], columns=['Lane', 'Tile', 'PCT_Occupied']))\n",
    "        q30PctDF = \\\n",
    "            get_pct_q30_score_per_tile(\n",
    "                qDF=df_dict.get(\"Q\"),\n",
    "                reads_stat=reads_stat)\n",
    "        tileClusterAndDensityDF = \\\n",
    "            get_cluster_and_density_per_tile(\n",
    "                tileDF=df_dict.get(\"Tile\"))\n",
    "        if 'Error' in df_dict:\n",
    "            errorRateTileDF = \\\n",
    "                get_mean_error_rate_per_tile(\n",
    "                    errorDF=df_dict.get(\"Error\"),\n",
    "                    reads_stat=reads_stat)\n",
    "        else:\n",
    "            errorRateTileDF = \\\n",
    "                spark.createDataFrame(\n",
    "                    pd.DataFrame([], columns=['Lane', 'Tile', 'mean_ErrorRate']),\n",
    "                schema=\"Lane INT, Tile INT, mean_ErrorRate FLOAT\")\n",
    "        meanCalledintensityDF = \\\n",
    "            get_mean_callout_intensities_per_tile(\n",
    "                correctedIntDF=df_dict.get(\"CorrectedInt\"),\n",
    "                reads_stat=reads_stat)\n",
    "        mergedDF = \\\n",
    "            merge_all_tile_data(\n",
    "                lane_count=lane_count,\n",
    "                intensityC1TileDF=intensityC1TileDF,\n",
    "                phaseCalcDF=phaseCalcDF,\n",
    "                tileOccupiedDF=tileOccupiedDF,\n",
    "                q30PctDF=q30PctDF,\n",
    "                tileClusterAndDensityDF=tileClusterAndDensityDF,\n",
    "                errorRateTileDF=errorRateTileDF,\n",
    "                meanCalledintensityDF=meanCalledintensityDF)\n",
    "        return mergedDF\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q30_and_yield_values(\n",
    "        qDF: DataFrame,\n",
    "        reads_stat: pd.DataFrame,\n",
    "        lane_count: int) -> \\\n",
    "            Tuple[float, list[dict], list[dict], float, list[dict], list[dict]]:\n",
    "    try:\n",
    "        avg_pct_q30 = 0.0\n",
    "        list_pct_q30_rg = []\n",
    "        list_pct_q30_rg_lane = []\n",
    "        final_yield = 0.0\n",
    "        list_yield_rg = []\n",
    "        list_yield_lane_rg = []\n",
    "        qDF = qDF.na.fill('0')\n",
    "        qDF = qDF.na.replace('nan', '0')\n",
    "        ## set q30 bins\n",
    "        qbin_count = \\\n",
    "            len([c for c in qDF.columns if c.startswith('Bin_')])\n",
    "        if qbin_count > 3:\n",
    "            qbin_list = [f\"sum(Bin_{c})\" for c in range(30, 51)]\n",
    "        else:\n",
    "            qbin_list = [\"sum(Bin_3)\",]\n",
    "        ## calculate avg q30\n",
    "        d = \\\n",
    "            qDF.\\\n",
    "            selectExpr(\n",
    "                *(f\"cast({c} as long) as {c}\" if c.startswith('Bin_') else f\"cast({c} as int)\" for c in qDF.columns)).\\\n",
    "            select(\n",
    "                *(sf.sum(col(c)) for c in qDF.columns if c.startswith('Bin_')),).\\\n",
    "            toPandas().\\\n",
    "            to_dict(orient='records')\n",
    "        total_bases = \\\n",
    "            sum([int(v) for k,v in d[0].items() if k!='Lane'])\n",
    "        q30_bases = \\\n",
    "            sum([int(v) for k,v in d[0].items()\n",
    "                     if k!='Lane' and k in qbin_list])\n",
    "        avg_q30 = \\\n",
    "            f\"{q30_bases/total_bases * 100 :.2f}\"\n",
    "        ## calculate total yield\n",
    "        # total_yield = total_bases/2\n",
    "        # final_yield = f'{total_yield/1000000000:.2f}' \n",
    "        ## rg wise sum\n",
    "        for entry in reads_stat.to_dict(orient='records'):\n",
    "            read_id = entry.get('read_id')\n",
    "            read_cycle = entry.get('cycles')\n",
    "            start_cycle = entry.get('start_cycle')\n",
    "            index_read = entry.get('index_read')\n",
    "            finish_cycle = entry.get('finish_cycle')\n",
    "            ## filter df for lane and cycle\n",
    "            temp_qDF = \\\n",
    "                qDF.\\\n",
    "                withColumn('Cycle', col('Cycle').cast('int')).\\\n",
    "                filter(col('Cycle')>=start_cycle).\\\n",
    "                filter(col('Cycle')<finish_cycle)\n",
    "            ## get count\n",
    "            d = \\\n",
    "                temp_qDF.\\\n",
    "                selectExpr(\n",
    "                    *(f\"cast({c} as long) as {c}\"\n",
    "                        if c.startswith('Bin_') else f\"cast({c} as int)\"\n",
    "                            for c in temp_qDF.columns)).\\\n",
    "                selectExpr(*(f\"sum({c})\" for c in temp_qDF.columns if c.startswith('Bin_'))).\\\n",
    "                toPandas().to_dict(orient='records')\n",
    "            total_bases = \\\n",
    "                sum([int(v) for k,v in d[0].items() if k!='Lane'])\n",
    "            q30_bases = \\\n",
    "                sum([int(v) for k,v in d[0].items()\n",
    "                        if k!='Lane' and k in qbin_list])\n",
    "            pct_q30 = \\\n",
    "                f\"{q30_bases/total_bases * 100 :.2f}\"\n",
    "            total_yield = total_bases/2\n",
    "            total_yield = f'{total_yield/1000000000:.2f}'\n",
    "            list_pct_q30_rg.append({'Read': read_id, 'PCT_Q30': pct_q30})\n",
    "            list_yield_rg.append({'Read': read_id, 'Yield': total_yield})\n",
    "            final_yield += float(total_yield)\n",
    "        ## lane and rg wise sum\n",
    "        for lane_id in range(1, lane_count+1):\n",
    "            for entry in reads_stat.to_dict(orient='records'):\n",
    "                read_id = entry.get('read_id')\n",
    "                read_cycle = entry.get('cycles')\n",
    "                start_cycle = entry.get('start_cycle')\n",
    "                index_read = entry.get('index_read')\n",
    "                finish_cycle = entry.get('finish_cycle')\n",
    "                ## filter df for lane and cycle\n",
    "                temp_qDF = \\\n",
    "                    qDF.\\\n",
    "                    withColumn('Cycle', col('Cycle').cast('int')).\\\n",
    "                    withColumn('Lane', col('Lane').cast('int')).\\\n",
    "                    filter(col('Lane')==lane_id).\\\n",
    "                    filter(col('Cycle')>=start_cycle).\\\n",
    "                    filter(col('Cycle')<finish_cycle)\n",
    "                ## get count\n",
    "                d = \\\n",
    "                    temp_qDF.\\\n",
    "                    selectExpr(\n",
    "                        *(f\"cast({c} as long) as {c}\"\n",
    "                              if c.startswith('Bin_') else f\"cast({c} as int)\"\n",
    "                                  for c in temp_qDF.columns)).\\\n",
    "                    groupBy('Lane').\\\n",
    "                    agg(\n",
    "                        *(sf.sum(col(c)) \n",
    "                              for c in temp_qDF.columns \n",
    "                                  if c.startswith('Bin_')),).\\\n",
    "                    toPandas().to_dict(orient='records')\n",
    "                total_bases = \\\n",
    "                    sum([int(v) for k,v in d[0].items() if k!='Lane'])\n",
    "                q30_bases = \\\n",
    "                    sum([int(v) for k,v in d[0].items()\n",
    "                         if k!='Lane' and k in qbin_list])\n",
    "                pct_q30 = \\\n",
    "                    f\"{q30_bases/total_bases * 100 :.2f}\"\n",
    "                total_yield = total_bases/2\n",
    "                total_yield = f'{total_yield/1000000000:.2f}'\n",
    "                list_pct_q30_rg_lane.append({'Lane': lane_id, 'Read': read_id, 'PCT_Q30': pct_q30})\n",
    "                list_yield_lane_rg.append({'Lane': lane_id, 'Read': read_id, 'Yield': total_yield})\n",
    "        return avg_q30, list_pct_q30_rg, list_pct_q30_rg_lane, final_yield, list_yield_rg, list_yield_lane_rg\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_stat = \\\n",
    "    pd.DataFrame([\n",
    "        {'read_id': 1, 'cycles': 2, 'start_cycle': 1, 'finish_cycle': 2, 'index_read': 'N'},\n",
    "        {'read_id': 2, 'cycles': 2, 'start_cycle': 3, 'finish_cycle': 4, 'index_read': 'N'}])\n",
    "t_df = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '1', 'Bin_1': '5564900', 'Bin_2': '8470200', 'Bin_3': '327516000'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '2', 'Bin_1': '5564900', 'Bin_2': '9656800', 'Bin_3': '326329300'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '3', 'Bin_1': '3028000', 'Bin_2': '6506100', 'Bin_3': '332017100'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '4', 'Bin_1': '3191600', 'Bin_2': '5810500', 'Bin_3': '332549000'}]))\n",
    "avg_q30, list_pct_q30_rg, list_pct_q30_rg_lane, final_yield, list_yield_rg, list_yield_lane_rg = \\\n",
    "    get_q30_and_yield_values(\n",
    "        qDF=t_df,\n",
    "        reads_stat=reads_stat,\n",
    "        lane_count=1)\n",
    "assert str(float(avg_q30)) == f\"{(327516000+332017100)/(5564900+8470200+327516000+3028000+6506100+332017100)*100:.1f}\"\n",
    "assert str(final_yield) == f\"{(5564900+8470200+327516000+3028000+6506100+332017100)/2/1000000000:.2f}\"\n",
    "assert str(list_pct_q30_rg[0].get('PCT_Q30')) == f\"{327516000/(5564900+8470200+327516000)*100:.2f}\"\n",
    "assert str(list_yield_rg[0].get('Yield')) == f\"{(5564900+8470200+327516000)/2/1000000000:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calculate_phasing_scores(\n",
    "        phaseDF: DataFrame,\n",
    "        reads_stat: pd.DataFrame) -> list:\n",
    "    try:\n",
    "        phaseDF = phaseDF.na.fill('0')\n",
    "        phaseDF = phaseDF.na.replace('nan', '0')\n",
    "        phasing_values = list()\n",
    "        for row in reads_stat.to_dict(orient='records'):\n",
    "            read_id = row.get('read_id')\n",
    "            start_cycle = row.get('start_cycle')\n",
    "            finish_cycle = row.get('finish_cycle')\n",
    "            index_read = row.get('index_read')\n",
    "            if index_read == 'N':\n",
    "                df = \\\n",
    "                phaseDF.\\\n",
    "                selectExpr(*(f\"cast({c} as float)\" for c in (\"Lane\", \"Tile\", \"Cycle\", \"Phasing\", \"Prephasing\"))).\\\n",
    "                filter(col(\"Cycle\")>=start_cycle).filter(col(\"Cycle\")<finish_cycle).\\\n",
    "                groupBy('Lane', 'Cycle').\\\n",
    "                agg(\n",
    "                    sf.sum(\"Phasing\").alias(\"total_phasing\"),\n",
    "                    sf.sum(\"Prephasing\").alias(\"total_prephasing\"),\n",
    "                    sf.mean(\"Phasing\").alias(\"mean_phasing\"),\n",
    "                    sf.mean(\"Prephasing\").alias(\"mean_prephasing\"),\n",
    "                    sf.median(\"Phasing\").alias(\"median_phasing\"),\n",
    "                    sf.median(\"Prephasing\").alias(\"median_prephasing\")).\\\n",
    "                toPandas()\n",
    "                for lane, l_data in df.groupby('Lane'):\n",
    "                    t_l_data = l_data.copy()\n",
    "                    t_l_data['Cycle'] = t_l_data['Cycle'] - (start_cycle-1)\n",
    "                    slope_p, offset_p = np.polyfit(t_l_data[\"Cycle\"], t_l_data[\"mean_phasing\"], 1)\n",
    "                    slope_pr, offset_pr = np.polyfit(t_l_data[\"Cycle\"], t_l_data[\"mean_prephasing\"], 1)\n",
    "                    row = {\n",
    "                        'Lane': lane,\n",
    "                        'Read': read_id,\n",
    "                        'Phasing_slope': slope_p,\n",
    "                        'Phasing_offset': offset_p,\n",
    "                        'Prephasing_slope': slope_pr,\n",
    "                        'Prephasing_offset': offset_pr}\n",
    "                    phasing_values.append(row)\n",
    "        return phasing_values\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_stat = \\\n",
    "    pd.DataFrame([\n",
    "        {'read_id': 1, 'cycles': 4, 'start_cycle': 1, 'finish_cycle': 4, 'index_read': 'N'},\n",
    "        {'read_id': 2, 'cycles': 4, 'start_cycle': 5, 'finish_cycle': 8, 'index_read': 'N'}])\n",
    "t_df = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': 1, 'Tile': 1101, 'Cycle': 1, 'Phasing': 3, 'Prephasing': 1},\n",
    "        {'Lane': 1, 'Tile': 1101, 'Cycle': 2, 'Phasing': 1.25, 'Prephasing': 0.75},\n",
    "        {'Lane': 1, 'Tile': 1101, 'Cycle': 3, 'Phasing': 1, 'Prephasing': 0.75},\n",
    "        {'Lane': 1, 'Tile': 1101, 'Cycle': 4, 'Phasing': 1.75, 'Prephasing': 1.5},\n",
    "        {'Lane': 1, 'Tile': 1101, 'Cycle': 5, 'Phasing': 2, 'Prephasing': 2},\n",
    "        {'Lane': 1, 'Tile': 1101, 'Cycle': 6, 'Phasing': 1, 'Prephasing': 1},\n",
    "        {'Lane': 1, 'Tile': 1101, 'Cycle': 7, 'Phasing': 1.25, 'Prephasing': 1.75},\n",
    "        {'Lane': 1, 'Tile': 1101, 'Cycle': 8, 'Phasing': 2, 'Prephasing': 2}]))\n",
    "phasing_values = \\\n",
    "    get_calculate_phasing_scores(\n",
    "        phaseDF=t_df,\n",
    "        reads_stat=reads_stat)\n",
    "phasing_values_df = \\\n",
    "    pd.DataFrame(phasing_values)\n",
    "read1 = \\\n",
    "    phasing_values_df[phasing_values_df['Read']==1]\n",
    "r1_p_s, r1_p_o = \\\n",
    "    np.polyfit([1, 2, 3], [3.0, 1.25, 1.0], 1)\n",
    "r1_pr_s, r1_pr_o = \\\n",
    "    np.polyfit([1, 2, 3], [1.0, 0.75, 0.75], 1)\n",
    "# assert read1['Phasing_slope'].values[0] == r1_p_s\n",
    "# assert read1['Phasing_offset'].values[0] == r1_p_o\n",
    "# assert read1['Prephasing_slope'].values[0] == r1_pr_s\n",
    "# assert read1['Prephasing_offset'].values[0] == r1_pr_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_pct_occupied(\n",
    "        tileDF: DataFrame,\n",
    "        etileDF: DataFrame) -> float:\n",
    "    try:\n",
    "        tileDF = tileDF.na.fill('0')\n",
    "        tileDF = tileDF.na.replace('nan', '0')\n",
    "        etileDF = etileDF.na.fill('0')\n",
    "        etileDF = etileDF.na.replace('nan', '0')\n",
    "        total_ClusterCount = \\\n",
    "            tileDF.\\\n",
    "            selectExpr(\"cast(Read as int)\", \"cast(ClusterCount as decimal)\").\\\n",
    "            filter(col('Read')==1).\\\n",
    "            select(sf.sum(\"ClusterCount\").alias(\"total_ClusterCount\")).\\\n",
    "            collect()\n",
    "        total_OccupiedCount = \\\n",
    "            etileDF.\\\n",
    "            selectExpr(\"cast(OccupiedCount as decimal)\").\\\n",
    "            select(sf.sum(\"OccupiedCount\").alias(\"total_OccupiedCount\")).\\\n",
    "            collect()\n",
    "        pct_occupied = \\\n",
    "            float(total_OccupiedCount[0][\"total_OccupiedCount\"]/total_ClusterCount[0][\"total_ClusterCount\"]*100)\n",
    "        return pct_occupied\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tileDF = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': 1, 'Tile': 1101, 'Read': 1, 'ClusterCount': 4091900},\n",
    "        {'Lane': 1, 'Tile': 1101, 'Read': 4, 'ClusterCount': 4091900}]))\n",
    "t_etileDF = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': 1, 'Tile': 1101, 'OccupiedCount': 3615010}]))\n",
    "pct_occupied = \\\n",
    "    get_total_pct_occupied(\n",
    "        tileDF=t_tileDF,\n",
    "        etileDF=t_etileDF)\n",
    "assert pct_occupied == 3615010/4091900*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intensity_cycle_1_value(\n",
    "        extDF: DataFrame,\n",
    "        reads_stat: pd.DataFrame) -> Tuple[list, list]:\n",
    "    try:\n",
    "        extDF = extDF.na.fill('0')\n",
    "        extDF = extDF.na.replace('nan', '0')\n",
    "        intensity_list = list()\n",
    "        intensity_rg_list = list()\n",
    "        if \"MaxIntensity_blue\" in extDF.columns:\n",
    "            intensity_col = \"MaxIntensity_blue\"\n",
    "        if \"MaxIntensity_RED\" in extDF.columns:\n",
    "            intensity_col = \"MaxIntensity_RED\"\n",
    "        if \"MaxIntensity_A\" in extDF.columns:\n",
    "            intensity_col = \"MaxIntensity_A\"\n",
    "        for row in reads_stat.to_dict(orient='records'):\n",
    "            read_id = row.get('read_id')\n",
    "            start_cycle = row.get('start_cycle')\n",
    "            pdf = \\\n",
    "                extDF.\\\n",
    "                filter(col(\"Cycle\")==start_cycle).\\\n",
    "                groupBy('Lane').\\\n",
    "                agg(sf.mean(intensity_col).alias(\"Intensity_cycle_1\")).\\\n",
    "                toPandas()\n",
    "            pdf['Read'] = read_id\n",
    "            intensity_list.extend(pdf.to_dict(orient='records'))\n",
    "            rg_pdf = \\\n",
    "                extDF.\\\n",
    "                filter(col(\"Cycle\")==start_cycle).\\\n",
    "                select(sf.mean(intensity_col).alias(\"Intensity_cycle_1\")).\\\n",
    "                toPandas()\n",
    "            rg_pdf['Read'] = read_id\n",
    "            intensity_rg_list.extend(rg_pdf.to_dict(orient='records'))\n",
    "        return intensity_list, intensity_rg_list\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '1', 'MaxIntensity_RED': '2264'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '2', 'MaxIntensity_RED': '2217'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '3', 'MaxIntensity_RED': '2295'},\n",
    "        {'Lane': '1', 'Tile': '1101', 'Cycle': '4', 'MaxIntensity_RED': '2211'}]))\n",
    "reads_stat = \\\n",
    "    pd.DataFrame([\n",
    "        {'read_id': 1, 'cycles': 2, 'start_cycle': 1, 'finish_cycle': 2, 'index_read': 'N'},\n",
    "        {'read_id': 2, 'cycles': 2, 'start_cycle': 3, 'finish_cycle': 4, 'index_read': 'N'}])\n",
    "intensity_list, intensity_rg_list = \\\n",
    "    get_intensity_cycle_1_value(\n",
    "        extDF=t_df,\n",
    "        reads_stat=reads_stat)\n",
    "intensity_list_df = pd.DataFrame(intensity_list)\n",
    "intensity_list_df_r1 = intensity_list_df[intensity_list_df['Read']==1]\n",
    "assert intensity_list_df_r1['Intensity_cycle_1'].values[0] == 2264\n",
    "intensity_rg_list_df = pd.DataFrame(intensity_rg_list)\n",
    "intensity_rg_list_df_r1 = intensity_rg_list_df[intensity_rg_list_df['Read']==1]\n",
    "assert intensity_rg_list_df_r1['Intensity_cycle_1'].values[0] == 2264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_rate_value(\n",
    "        eDF: DataFrame,\n",
    "        reads_stat: pd.DataFrame) -> Tuple[list, list]:\n",
    "    try:\n",
    "        eDF = eDF.na.fill('0')\n",
    "        eDF = eDF.na.replace('nan', '0')\n",
    "        error_rate_list = list()\n",
    "        error_rate_rg_list = list()\n",
    "        for row in reads_stat.to_dict(orient='records'):\n",
    "            read_id = row.get('read_id')\n",
    "            start_cycle = row.get('start_cycle')\n",
    "            finish_cycle = row.get('finish_cycle')\n",
    "            index_read = row.get('index_read')\n",
    "            if index_read == 'N':\n",
    "                pdf = \\\n",
    "                    eDF.\\\n",
    "                    selectExpr(\n",
    "                        \"cast(Lane as int)\",\n",
    "                        \"cast(Tile as int)\",\n",
    "                        \"cast(Cycle as int)\",\n",
    "                        \"cast(ErrorRate as double)\").\\\n",
    "                    filter(col(\"Cycle\")>=start_cycle).\\\n",
    "                    filter(col(\"Cycle\")<finish_cycle).\\\n",
    "                    groupBy('Lane').\\\n",
    "                    agg(sf.mean(\"ErrorRate\").alias(\"ErrorRate\")).\\\n",
    "                    toPandas()\n",
    "                pdf['Read'] = read_id\n",
    "                error_rate_list.extend(pdf.to_dict(orient='records'))\n",
    "                rg_pdf = \\\n",
    "                    eDF.\\\n",
    "                    selectExpr(\n",
    "                        \"cast(Lane as int)\",\n",
    "                        \"cast(Tile as int)\",\n",
    "                        \"cast(Cycle as int)\",\n",
    "                        \"cast(ErrorRate as double)\").\\\n",
    "                    filter(col(\"Cycle\")>=start_cycle).\\\n",
    "                    filter(col(\"Cycle\")<finish_cycle).\\\n",
    "                    select(sf.mean(\"ErrorRate\").alias(\"ErrorRate\")).\\\n",
    "                    toPandas()\n",
    "                rg_pdf['Read'] = read_id\n",
    "                error_rate_rg_list.extend(rg_pdf.to_dict(orient='records'))\n",
    "        return error_rate_list, error_rate_rg_list\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': 1, 'Tile': 2103, 'Cycle': 1, 'ErrorRate': 0.17},\n",
    "        {'Lane': 1, 'Tile': 2103, 'Cycle': 2, 'ErrorRate': 0.05},\n",
    "        {'Lane': 1, 'Tile': 2103, 'Cycle': 3, 'ErrorRate': 0.02},\n",
    "        {'Lane': 1, 'Tile': 2103, 'Cycle': 4, 'ErrorRate': 0.05}]))\n",
    "reads_stat = \\\n",
    "    pd.DataFrame([\n",
    "        {'read_id': 1, 'cycles': 4, 'start_cycle': 1, 'finish_cycle': 2, 'index_read': 'N'},\n",
    "        {'read_id': 2, 'cycles': 4, 'start_cycle': 3, 'finish_cycle': 8, 'index_read': 'N'}])\n",
    "error_rate_list, error_rate_rg_list = \\\n",
    "    get_error_rate_value(\n",
    "        eDF=t_df,\n",
    "        reads_stat=reads_stat)\n",
    "error_rate_list_df = pd.DataFrame(error_rate_list)\n",
    "error_rate_list_df_r1 = error_rate_list_df[error_rate_list_df['Read']==1]\n",
    "assert error_rate_list_df_r1['ErrorRate'].values[0] == 0.17\n",
    "error_rate_rg_list_df = pd.DataFrame(error_rate_rg_list)\n",
    "error_rate_rg_list_df_r1 = error_rate_rg_list_df[error_rate_rg_list_df['Read']==1]\n",
    "assert error_rate_rg_list_df_r1['ErrorRate'].values[0] == 0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phix_aligned_value(\n",
    "        tileDF: DataFrame,\n",
    "        reads_stat: pd.DataFrame) -> Tuple[list, list]:\n",
    "    try:\n",
    "        tileDF = tileDF.na.fill('0')\n",
    "        tileDF = tileDF.na.replace('nan', '0')\n",
    "        aligned_list = list()\n",
    "        aligned_rg_list = list()\n",
    "        for row in reads_stat.to_dict(orient='records'):\n",
    "            read_id = row.get('read_id')\n",
    "            index_read = row.get('index_read')\n",
    "            if index_read == 'N':\n",
    "                pdf = \\\n",
    "                    tileDF.\\\n",
    "                    selectExpr(\"cast(Lane as int)\", \"cast(Read as int)\", \"cast(Aligned as float)\").\\\n",
    "                    filter(col(\"Read\")==read_id).\\\n",
    "                    filter(col(\"Aligned\")>0).\\\n",
    "                    groupBy(\"Lane\").\\\n",
    "                    agg(sf.mean(\"Aligned\").alias(\"Aligned\")).\\\n",
    "                    toPandas()\n",
    "                pdf['Read'] = read_id\n",
    "                aligned_list.extend(pdf.to_dict(orient='records'))\n",
    "                rg_pdf = \\\n",
    "                    tileDF.\\\n",
    "                    selectExpr(\"cast(Read as int)\", \"cast(Aligned as float)\").\\\n",
    "                    filter(col(\"Read\")==read_id).\\\n",
    "                    filter(col(\"Aligned\")>0).\\\n",
    "                    select(sf.mean(\"Aligned\").alias(\"Aligned\")).\\\n",
    "                    toPandas()\n",
    "                rg_pdf['Read'] = read_id\n",
    "                aligned_rg_list.extend(rg_pdf.to_dict(orient='records'))\n",
    "        return aligned_list, aligned_rg_list\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_and_density_values(\n",
    "        tileDF: DataFrame) -> Tuple[float, list]:\n",
    "    try:\n",
    "        tileDF = tileDF.na.fill('0')\n",
    "        tileDF = tileDF.na.replace('nan', '0')\n",
    "        cluster_df = \\\n",
    "            tileDF.\\\n",
    "            selectExpr(*(f\"cast({c} as decimal)\" \\\n",
    "                         for c in (\"Lane\", \"Read\", \"ClusterCount\", \"ClusterCountPF\", \"Density\", \"DensityPF\"))).\\\n",
    "            groupBy(\"Lane\", \"Read\").\\\n",
    "            agg(\n",
    "                sf.sum(\"ClusterCount\").alias(\"total_ClusterCount\"),\n",
    "                sf.sum(\"ClusterCountPF\").alias(\"total_ClusterCountPF\"),\n",
    "                (sf.sum(\"ClusterCountPF\")/sf.sum(\"ClusterCount\") * 100).alias('PCT_ClusterCountPF'),\n",
    "                sf.sum(\"Density\").alias(\"total_Density\"),\n",
    "                sf.sum(\"DensityPF\").alias(\"total_DensityPF\")\n",
    "            ).\\\n",
    "            toPandas()\n",
    "        total_cluster_pf = \\\n",
    "            float(tileDF.\\\n",
    "                selectExpr(*(f\"cast({c} as decimal)\" \\\n",
    "                             for c in (\"Lane\", \"Read\", \"ClusterCount\", \"ClusterCountPF\", \"Density\", \"DensityPF\"))).\\\n",
    "                selectExpr(\"sum(ClusterCountPF)/sum(ClusterCount)*100 as total_cluster_pf\").collect()[0]['total_cluster_pf'])\n",
    "        return total_cluster_pf, cluster_df.astype(float).to_dict(orient='records')\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = \\\n",
    "    spark.createDataFrame(pd.DataFrame([\n",
    "        {'Lane': 1, 'Tile': 1101, 'Read': 1, 'ClusterCount': 4091900, 'ClusterCountPF': 3430010, 'Density': 2961260, 'DensityPF': 2482260},\n",
    "        {'Lane': 1, 'Tile': 1101, 'Read': 4, 'ClusterCount': 4091900, 'ClusterCountPF': 3430010, 'Density': 2961260, 'DensityPF': 2482260}]))\n",
    "total_cluster_pf, cluster_df_list = \\\n",
    "    get_cluster_and_density_values(\n",
    "        tileDF=t_df)\n",
    "assert f\"{total_cluster_pf:.2f}\" == f\"{(3430010*2)/(4091900*2)*100:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_report(\n",
    "        metrics_list: list,\n",
    "        runinfo_xml: str,\n",
    "        runparameters_xml: str) -> Tuple[list, list, dict]:\n",
    "    try:\n",
    "        ## read runinfo file and get reads_stat\n",
    "        run_id, flowcell_id, instrument_id, lane_count, reads_stat, total_cycles = \\\n",
    "            read_runinfo_xml(runinfo_xml)\n",
    "        ## read runparameters_xml and get flowcell_mode\n",
    "        flowcell_mode = \\\n",
    "            read_runparameters_xml(\n",
    "                runparametersXml_path=runparameters_xml)\n",
    "        ## read interop metrics\n",
    "        df_dict = get_interop_data_frames(metrics_list)\n",
    "        ## get yield data\n",
    "        avg_q30, list_pct_q30_rg, list_pct_q30_rg_lane, final_yield, list_yield_rg, list_yield_lane_rg = \\\n",
    "            get_q30_and_yield_values(\n",
    "                qDF=df_dict.get('Q'),\n",
    "                reads_stat=reads_stat,\n",
    "                lane_count=lane_count)\n",
    "        ## get cluster and density values\n",
    "        final_cluster_pf, cluster_values = \\\n",
    "            get_cluster_and_density_values(\n",
    "            tileDF=df_dict.get('Tile'))\n",
    "        ## get intensity values\n",
    "        indensity_c1_data, indensity_c1_rg_data = \\\n",
    "            get_intensity_cycle_1_value(\n",
    "                extDF=df_dict.get('Extraction'),\n",
    "                reads_stat=reads_stat)\n",
    "        ## get error data\n",
    "        if 'Error' in df_dict:\n",
    "            errorRate_data,  errorRate_rg_data = \\\n",
    "                get_error_rate_value(\n",
    "                    eDF=df_dict.get('Error'),\n",
    "                    reads_stat=reads_stat)\n",
    "        else:\n",
    "            errorRate_data = \\\n",
    "                pd.DataFrame([], columns=['ErrorRate', 'Read', 'Lane']).\\\n",
    "                to_dict(orient='records')\n",
    "            errorRate_rg_data = \\\n",
    "                pd.DataFrame([], columns=['ErrorRate', 'Read']).\\\n",
    "                to_dict(orient='records')\n",
    "        ## get pct occupied data\n",
    "        if 'ExtendedTile' in df_dict:\n",
    "            pct_occupied = \\\n",
    "                get_total_pct_occupied(\n",
    "                    tileDF=df_dict.get('Tile'),\n",
    "                    etileDF=df_dict.get('ExtendedTile'))\n",
    "        else:\n",
    "            pct_occupied = \"0.0\"\n",
    "        ## get phasing data\n",
    "        if 'EmpiricalPhasing' in df_dict:\n",
    "            phasing_data = \\\n",
    "                get_calculate_phasing_scores(\n",
    "                    phaseDF=df_dict.get('EmpiricalPhasing'),\n",
    "                    reads_stat=reads_stat)\n",
    "        else:\n",
    "            phasing_data = \\\n",
    "                pd.DataFrame([], columns=['Lane', 'Read']).\\\n",
    "                to_dict(orient='records')\n",
    "        ## get aligned data\n",
    "        aligned_data, aligned_rg_data = \\\n",
    "            get_phix_aligned_value(\n",
    "                tileDF=df_dict.get('Tile'),\n",
    "                reads_stat=reads_stat)\n",
    "        ## Combine data for overview table\n",
    "        # * Actual yeild\n",
    "        # * Avg Q30 PCT\n",
    "        # * PCT Cluster PF\n",
    "        # * PCT Occupied\n",
    "        # * Total Cycles\n",
    "        overview_data = [{\n",
    "            \"Actual yield\": final_yield,\n",
    "            \"Avg Q30 PCT\": avg_q30,\n",
    "            \"PCT Cluster PF\": final_cluster_pf,\n",
    "            \"PCT Occupied\": pct_occupied,\n",
    "            \"Total Cycles\": total_cycles,\n",
    "            \"Flowcell mode\": flowcell_mode\n",
    "        }]\n",
    "        ## Combine data for RG overview table\n",
    "        # * yield\n",
    "        # * PCT Q30\n",
    "        # * aligned\n",
    "        # * error\n",
    "        # * intensity c1\n",
    "        rg_qc = \\\n",
    "            combine_rg_level_info_for_report(\n",
    "                reads_stat=reads_stat,\n",
    "                list_pct_q30_rg=list_pct_q30_rg,\n",
    "                list_yield_rg=list_yield_rg,\n",
    "                errorRate_rg_data=errorRate_rg_data,\n",
    "                aligned_rg_data=aligned_rg_data,\n",
    "                indensity_c1_rg_data=indensity_c1_rg_data)\n",
    "        ## combine data for RG Lane overview table\n",
    "        lane_qc = \\\n",
    "            combine_rg_and_lane_level_info_for_report(\n",
    "                lane_count=lane_count,\n",
    "                reads_stat=reads_stat,\n",
    "                list_pct_q30_rg_lane=list_pct_q30_rg_lane,\n",
    "                list_yield_lane_rg=list_yield_lane_rg,\n",
    "                cluster_values=cluster_values,\n",
    "                indensity_c1_data=indensity_c1_data,\n",
    "                errorRate_data=errorRate_data,\n",
    "                aligned_data=aligned_data,\n",
    "                phasing_data=phasing_data)\n",
    "        return overview_data, rg_qc, lane_qc, df_dict\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_rg_level_info_for_report(\n",
    "        reads_stat: pd.DataFrame,\n",
    "        list_pct_q30_rg: list,\n",
    "        list_yield_rg: list,\n",
    "        errorRate_rg_data: list,\n",
    "        aligned_rg_data: list,\n",
    "        indensity_c1_rg_data: list) \\\n",
    "        -> list:\n",
    "    try:\n",
    "        rg_qc = list()\n",
    "        list_pct_q30_rg_df = \\\n",
    "            pd.DataFrame(list_pct_q30_rg)\n",
    "        list_yield_rg_df = \\\n",
    "            pd.DataFrame(list_yield_rg)\n",
    "        if len(errorRate_rg_data) > 0:\n",
    "            errorRate_rg_df = \\\n",
    "                pd.DataFrame(errorRate_rg_data)\n",
    "        else:\n",
    "            errorRate_rg_df = \\\n",
    "                pd.DataFrame([], columns=['Read', 'ErrorRate'])\n",
    "        if len(aligned_rg_data) > 0:\n",
    "            aligned_rg_df = \\\n",
    "                pd.DataFrame(aligned_rg_data)\n",
    "        else:\n",
    "            aligned_rg_df = \\\n",
    "                pd.DataFrame([], columns=['Read', 'Aligned'])\n",
    "        indensity_c1_rg_df = \\\n",
    "            pd.DataFrame(indensity_c1_rg_data)\n",
    "        \n",
    "        for row in reads_stat.to_dict(orient='records'):\n",
    "            rg_dict = dict()\n",
    "            read_id = row.get('read_id')\n",
    "            rg_dict.update({'Read': read_id})\n",
    "            ## get PCT Q30\n",
    "            pct_q30 = \\\n",
    "                list_pct_q30_rg_df[list_pct_q30_rg_df['Read']==read_id]['PCT_Q30'].\\\n",
    "                values.tolist()\n",
    "            if len(pct_q30) > 0:\n",
    "                pct_q30 = pct_q30[0]\n",
    "            else:\n",
    "                pct_q30 = 0\n",
    "            rg_dict.update({'PCT_Q30': pct_q30})\n",
    "            ## get yield\n",
    "            total_yeild = \\\n",
    "                list_yield_rg_df[list_yield_rg_df['Read']==read_id]['Yield'].\\\n",
    "                values.tolist()\n",
    "            if len(total_yeild)> 0:\n",
    "                total_yeild = total_yeild[0]\n",
    "            else:\n",
    "                total_yeild = 0\n",
    "            rg_dict.update({'Yield': total_yeild})\n",
    "            ## get error rate\n",
    "            error_rate = \\\n",
    "                errorRate_rg_df[errorRate_rg_df['Read']==read_id]['ErrorRate'].\\\n",
    "                values.tolist()\n",
    "            if len(error_rate) > 0:\n",
    "                error_rate = error_rate[0]\n",
    "            else:\n",
    "                error_rate = 0\n",
    "            rg_dict.update({'ErrorRate': error_rate})\n",
    "            ## get aligned rate\n",
    "            aligned = \\\n",
    "                aligned_rg_df[aligned_rg_df['Read']==read_id]['Aligned'].\\\n",
    "                values.tolist()\n",
    "            if len(aligned) > 0:\n",
    "                aligned = aligned[0]\n",
    "            else:\n",
    "                aligned = 0\n",
    "            rg_dict.update({'Aligned': aligned})\n",
    "            ## get intensity c1\n",
    "            indensity_c1 = \\\n",
    "                indensity_c1_rg_df[indensity_c1_rg_df['Read']==read_id]['Intensity_cycle_1'].\\\n",
    "                values.tolist()\n",
    "            if len(indensity_c1) > 0:\n",
    "                indensity_c1 = indensity_c1[0]\n",
    "            else:\n",
    "                indensity_c1 = 0\n",
    "            rg_dict.update({'Intensity_cycle_1': indensity_c1})\n",
    "            rg_qc.append(rg_dict)\n",
    "        return rg_qc\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_rg_and_lane_level_info_for_report(\n",
    "        lane_count: int,\n",
    "        reads_stat: pd.DataFrame,\n",
    "        list_pct_q30_rg_lane: list,\n",
    "        list_yield_lane_rg: list,\n",
    "        cluster_values: list,\n",
    "        indensity_c1_data: list,\n",
    "        errorRate_data: list,\n",
    "        aligned_data: list,\n",
    "        phasing_data: list) \\\n",
    "        -> dict:\n",
    "    try:\n",
    "        lane_qc = dict()\n",
    "        list_pct_q30_rg_lane_df = \\\n",
    "            pd.DataFrame(list_pct_q30_rg_lane)\n",
    "        list_yield_lane_rg_df = \\\n",
    "            pd.DataFrame(list_yield_lane_rg)\n",
    "        cluster_values_df = \\\n",
    "            pd.DataFrame(cluster_values)\n",
    "        indensity_c1_df = \\\n",
    "            pd.DataFrame(indensity_c1_data)\n",
    "        if len(errorRate_data) > 0:\n",
    "            errorRate_df = \\\n",
    "                pd.DataFrame(errorRate_data)\n",
    "        else:\n",
    "            errorRate_df = \\\n",
    "                pd.DataFrame([], columns=['ErrorRate', 'Read', 'Lane'])\n",
    "        if len(aligned_data) > 0 :\n",
    "            aligned_df = \\\n",
    "                pd.DataFrame(aligned_data)\n",
    "        else:\n",
    "            aligned_df = \\\n",
    "                pd.DataFrame([], columns=['Lane', 'Read', 'Aligned'])\n",
    "        phasing_df = \\\n",
    "            pd.DataFrame(phasing_data)\n",
    "        for lane_id in range(1, lane_count+1):\n",
    "            lane_qc_list = list()\n",
    "            for row in reads_stat.to_dict(orient='records'):\n",
    "                rg_dict = dict()\n",
    "                read_id = row.get('read_id')\n",
    "                rg_dict.update({'Read': read_id})\n",
    "                ## get pct q30\n",
    "                pct_q30 = \\\n",
    "                    list_pct_q30_rg_lane_df[\n",
    "                        (list_pct_q30_rg_lane_df['Lane']==lane_id)& \\\n",
    "                        (list_pct_q30_rg_lane_df['Read']==read_id)]['PCT_Q30'].\\\n",
    "                    values.tolist()\n",
    "                if len(pct_q30) > 0:\n",
    "                    pct_q30 = pct_q30[0]\n",
    "                else:\n",
    "                    pct_q30 = 0\n",
    "                rg_dict.update({'PCT_Q30': pct_q30})\n",
    "                ## get yield\n",
    "                total_yield = \\\n",
    "                list_yield_lane_rg_df[\n",
    "                    (list_yield_lane_rg_df['Read']==read_id)& \\\n",
    "                    (list_yield_lane_rg_df['Lane']==lane_id)]['Yield'].\\\n",
    "                values.tolist()\n",
    "                if len(total_yield) > 0:\n",
    "                    total_yield = total_yield[0]\n",
    "                else:\n",
    "                    total_yield = 0\n",
    "                rg_dict.update({'Yield': total_yield})\n",
    "                ## get cluster pf values\n",
    "                cluster_lists = \\\n",
    "                    cluster_values_df[\n",
    "                        (cluster_values_df['Read'].astype(int)==read_id)& \\\n",
    "                        (cluster_values_df['Lane'].astype(int)==lane_id)]\n",
    "                if len(cluster_lists.index) > 0:\n",
    "                    pct_clusterCountPF = \\\n",
    "                        cluster_lists['PCT_ClusterCountPF'].values.tolist()[0]\n",
    "                else:\n",
    "                    pct_clusterCountPF = 0\n",
    "                rg_dict.update({'PCT_ClusterCountPF': pct_clusterCountPF})\n",
    "                ## intensity c1\n",
    "                indensity_c1 = \\\n",
    "                    indensity_c1_df[\n",
    "                        (indensity_c1_df['Lane'].astype(int)==lane_id)& \\\n",
    "                        (indensity_c1_df['Read'].astype(int)==read_id)]['Intensity_cycle_1'].\\\n",
    "                    values.tolist()\n",
    "                if len(indensity_c1) > 0:\n",
    "                    indensity_c1 = indensity_c1[0]\n",
    "                else:\n",
    "                    indensity_c1 = 0\n",
    "                rg_dict.update({'Intensity_cycle_1': indensity_c1})\n",
    "                ## error rate\n",
    "                errorRate = \\\n",
    "                    errorRate_df[\n",
    "                        (errorRate_df['Lane'].astype(int)==lane_id)& \\\n",
    "                        (errorRate_df['Read'].astype(int)==read_id)]['ErrorRate'].\\\n",
    "                    values.tolist()\n",
    "                if len(errorRate) > 0:\n",
    "                    errorRate = errorRate[0]\n",
    "                else:\n",
    "                    errorRate = 0\n",
    "                rg_dict.update({'ErrorRate': errorRate})\n",
    "                ## get aligned data\n",
    "                aligned = \\\n",
    "                    aligned_df[\n",
    "                        (aligned_df['Lane']==lane_id)& \\\n",
    "                        (aligned_df['Read']==read_id)]['Aligned'].\\\n",
    "                    values.tolist()\n",
    "                if len(aligned) > 0:\n",
    "                    aligned = aligned[0]\n",
    "                else:\n",
    "                    aligned = 0\n",
    "                rg_dict.update({'Aligned': aligned})\n",
    "                ## phasing data\n",
    "                phasing_slope = 0\n",
    "                phasing_offset = 0\n",
    "                prephasing_slope = 0\n",
    "                prephasing_offset = 0\n",
    "                if len(phasing_df.index) > 0:\n",
    "                    phasing = \\\n",
    "                        phasing_df[\n",
    "                            (phasing_df['Read']==read_id)& \\\n",
    "                            (phasing_df['Lane']==lane_id)]\n",
    "                    if len(phasing.index) > 0:\n",
    "                        phasing_slope = phasing[\"Phasing_slope\"].values.tolist()[0]\n",
    "                        phasing_offset = phasing[\"Phasing_offset\"].values.tolist()[0]\n",
    "                        prephasing_slope = phasing[\"Prephasing_slope\"].values.tolist()[0]\n",
    "                        prephasing_offset = phasing[\"Prephasing_offset\"].values.tolist()[0]\n",
    "                rg_dict.update({'Phasing_slope': phasing_slope})\n",
    "                rg_dict.update({'Phasing_offset': phasing_offset})\n",
    "                rg_dict.update({'Prephasing_slope': prephasing_slope})\n",
    "                rg_dict.update({'Prephasing_offset': prephasing_offset})\n",
    "                lane_qc_list.append(rg_dict)\n",
    "            lane_qc.update({lane_id: lane_qc_list})\n",
    "        return lane_qc\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePCA_for_tiles(df: pd.DataFrame) -> tuple[alt.Chart, alt.Chart]:\n",
    "    try:\n",
    "        df['Lane'] = df['Lane'].astype(int)\n",
    "        df['PCT_ClusterCountPF'] = df['PCT_ClusterCountPF'].astype(float)\n",
    "        df['PCT_DensityPF'] = df['PCT_DensityPF'].astype(float)\n",
    "        df['PCT_Q30'] = df['PCT_Q30'].astype(float)\n",
    "        df['PCT_Occupied'] = df['PCT_Occupied'].astype(float)\n",
    "        df['slope_p'] = df['slope_p'].astype(float)\n",
    "        df['mean_CalledCount_A'] = df['mean_CalledCount_A'].astype(int)\n",
    "        df['mean_CalledCount_T'] = df['mean_CalledCount_T'].astype(int)\n",
    "        df['mean_CalledCount_G'] = df['mean_CalledCount_G'].astype(int)\n",
    "        df['mean_CalledCount_C'] = df['mean_CalledCount_C'].astype(int)\n",
    "        df['intensity_c1'] = df['intensity_c1'].astype(int)\n",
    "        sub_cols = [\n",
    "            'PCT_ClusterCountPF', 'PCT_DensityPF', 'mean_CalledCount_A', \n",
    "            'mean_CalledCount_T', 'mean_CalledCount_G', 'mean_CalledCount_C', \n",
    "            'PCT_Q30', 'mean_ErrorRate', 'PCT_Occupied', 'intensity_c1', \n",
    "            'slope_p', 'offset_p', 'slope_pr', 'offset_pr']\n",
    "        sub_df = df[sub_cols].copy()\n",
    "        X_reduced = \\\n",
    "            PCA(n_components=2).fit(sub_df).fit_transform(sub_df)\n",
    "        pca_result = pd.DataFrame(X_reduced, columns=['pca1', 'pca2'])\n",
    "        sub_df['ClusterCountPF_status'] = np.where(df['PCT_ClusterCountPF'] < 0.7, 'Low', 'High')\n",
    "        pca_result['Lane'] = df['Lane'].astype(str)\n",
    "        pca_result['Tile'] = df['Tile'].astype(str)\n",
    "        pca_result['ClusterCountPF_status'] = sub_df['ClusterCountPF_status']\n",
    "        chart1 = \\\n",
    "            alt.Chart(pca_result, title=\"PCA plot - color by lane\").mark_point().encode(\n",
    "                x='pca1:Q',\n",
    "                y='pca2:Q',\n",
    "                color='Lane',\n",
    "                tooltip=['Lane:N', 'Tile:N']).\\\n",
    "            interactive().\\\n",
    "            properties(\n",
    "                width=900, height=400)\n",
    "        chart2 = \\\n",
    "            alt.Chart(pca_result, title=\"PCA plot - color by ClusterCountPF status\").mark_point().encode(\n",
    "                x='pca1:Q',\n",
    "                y='pca2:Q',\n",
    "                color='ClusterCountPF_status:N',\n",
    "                tooltip=['Lane:N', 'Tile:N', 'ClusterCountPF_status:N']).\\\n",
    "            interactive().\\\n",
    "            properties(\n",
    "                width=900, height=400)\n",
    "        return chart1, chart2\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hist_axis_from_tile_id(s: pd.Series)-> pd.Series:\n",
    "    tile = s['Tile']\n",
    "    s['h_x'] = int(str(int(tile))[0])\n",
    "    s['h_y'] = int(str(int(tile))[1:])\n",
    "    return s\n",
    "\n",
    "def get_tile_plots(df: pd.DataFrame) -> \\\n",
    "        Tuple[dict[str, list[alt.Chart]], dict[str, list[alt.Chart]], dict[str, list[alt.Chart]], dict[str, list[alt.Chart]], dict[str, list[alt.Chart]]]:\n",
    "    try:\n",
    "        df = \\\n",
    "            df.astype({\n",
    "                'Lane': int,\n",
    "                'Tile': int,\n",
    "                'PCT_ClusterCountPF': float,\n",
    "                'PCT_DensityPF': float,\n",
    "                'PCT_Q30': float,\n",
    "                'PCT_Occupied': float,\n",
    "                'slope_p': float,\n",
    "                'mean_CalledCount_A': int,\n",
    "                'mean_CalledCount_T': int,\n",
    "                'mean_CalledCount_G': int,\n",
    "                'mean_CalledCount_C': int,\n",
    "                'intensity_c1': int})\n",
    "        df = \\\n",
    "            df.apply(lambda s: add_hist_axis_from_tile_id(s), axis=1)\n",
    "        ## get PCT_ClusterCountPF plots\n",
    "        tilePCTClusterCountPFPlots = list()\n",
    "        for lane, l_data in df.groupby('Lane'):\n",
    "            chart1 = \\\n",
    "                alt.Chart(l_data, title=f\"Histogram plot for % ClusterCountPF per tile - Lane {int(lane)}\").mark_rect().encode(\n",
    "                    x=alt.X('h_y:O', axis=alt.Axis(labels=False, ticks=False, title=None)),\n",
    "                    y=alt.Y('h_x:O', axis=alt.Axis(labels=False, ticks=False, title=None)),\n",
    "                    color=alt.Color('PCT_ClusterCountPF:Q').scale(scheme='purpleblue'),\n",
    "                    tooltip=['Lane:N', 'Tile:O', 'PCT_ClusterCountPF:Q']\n",
    "                ).configure_view(\n",
    "                    step=13,\n",
    "                    strokeWidth=0\n",
    "                ).configure_axis(\n",
    "                    domain=False\n",
    "                ).properties(\n",
    "                    width=1080,\n",
    "                    height=100)\n",
    "            tilePCTClusterCountPFPlots.append(chart1)\n",
    "        ## get PCT_DensityPF plots\n",
    "        tilePCTDensityPFPlots = list()\n",
    "        for lane, l_data in df.groupby('Lane'):\n",
    "            chart2 = \\\n",
    "                alt.Chart(l_data, title=f\"Histogram plot for % DensityPF per tile - Lane {int(lane)}\").mark_rect().encode(\n",
    "                    x=alt.X('h_y:O', axis=alt.Axis(labels=False, ticks=False, title=None)),\n",
    "                    y=alt.Y('h_x:O', axis=alt.Axis(labels=False, ticks=False, title=None)),\n",
    "                    color=alt.Color('PCT_DensityPF:Q').scale(scheme='purpleblue'),\n",
    "                    tooltip=['Lane:N', 'Tile:O', 'PCT_DensityPF:Q']\n",
    "                ).configure_view(\n",
    "                    step=13,\n",
    "                    strokeWidth=0\n",
    "                ).configure_axis(\n",
    "                    domain=False\n",
    "                ).properties(\n",
    "                    width=1080,\n",
    "                    height=100)\n",
    "            tilePCTDensityPFPlots.append(chart2)\n",
    "        ## get PCT_Q30 plots\n",
    "        tilePCTQ30Plots = list()\n",
    "        for lane, l_data in df.groupby('Lane'):\n",
    "            chart3 = \\\n",
    "                alt.Chart(l_data, title=f\"Histogram plot for % Q30 per tile - Lane {int(lane)}\").mark_rect().encode(\n",
    "                    x=alt.X('h_y:O', axis=alt.Axis(labels=False, ticks=False, title=None)),\n",
    "                    y=alt.Y('h_x:O', axis=alt.Axis(labels=False, ticks=False, title=None)),\n",
    "                    color=alt.Color('PCT_Q30:Q').scale(scheme='purpleblue'),\n",
    "                    tooltip=['Lane:N', 'Tile:O', 'PCT_Q30:Q']\n",
    "                ).configure_view(\n",
    "                    step=13,\n",
    "                    strokeWidth=0\n",
    "                ).configure_axis(\n",
    "                    domain=False\n",
    "                ).properties(\n",
    "                    width=1080,\n",
    "                    height=100)\n",
    "            tilePCTQ30Plots.append(chart3)\n",
    "        ## get PCT_Occupied plots\n",
    "        tilePCTOccupiedPlots = list()\n",
    "        for lane, l_data in df.groupby('Lane'):\n",
    "            chart4 = \\\n",
    "                alt.Chart(l_data, title=f\"Histogram plot for % Occupied per tile - Lane {int(lane)}\").mark_rect().encode(\n",
    "                    x=alt.X('h_y:O', axis=alt.Axis(labels=False, ticks=False, title=None)),\n",
    "                    y=alt.Y('h_x:O', axis=alt.Axis(labels=False, ticks=False, title=None)),\n",
    "                    color=alt.Color('PCT_Occupied:Q').scale(scheme='purpleblue'),\n",
    "                    tooltip=['Lane:N', 'Tile:O','PCT_Occupied:Q']\n",
    "                ).configure_view(\n",
    "                    step=13,\n",
    "                    strokeWidth=0\n",
    "                ).configure_axis(\n",
    "                    domain=False\n",
    "                ).properties(\n",
    "                    width=1080,\n",
    "                    height=100)\n",
    "            tilePCTOccupiedPlots.append(chart4)\n",
    "        ## get intensity_c1 plots\n",
    "        tileIntensityC1Plots = list()\n",
    "        for lane, l_data in df.groupby('Lane'):\n",
    "            chart5 = \\\n",
    "                alt.Chart(l_data, title=f\"Histogram plot for Intensity cycle 1 per tile - Lane {int(lane)}\").mark_rect().encode(\n",
    "                    x=alt.X('h_y:O', axis=alt.Axis(labels=False, ticks=False, title=None)),\n",
    "                    y=alt.Y('h_x:O', axis=alt.Axis(labels=False, ticks=False, title=None)),\n",
    "                    color=alt.Color('intensity_c1:Q').scale(scheme='purpleblue'),\n",
    "                    tooltip=['Lane:N', 'Tile:O', 'intensity_c1:Q']\n",
    "                ).configure_view(\n",
    "                    step=13,\n",
    "                    strokeWidth=0\n",
    "                ).configure_axis(\n",
    "                    domain=False\n",
    "                ).properties(\n",
    "                    width=1080,\n",
    "                    height=100)\n",
    "            tileIntensityC1Plots.append(chart5)\n",
    "        return  {HTML(\"<h3>PCT ClusterCountPF</h3>\"): tilePCTClusterCountPFPlots}, \\\n",
    "                {HTML(\"<h3>PCT DensityPF</h3>\"): tilePCTDensityPFPlots}, \\\n",
    "                {HTML(\"<h3>PCT Q30</h3>\"): tilePCTQ30Plots}, \\\n",
    "                {HTML(\"<h3>PCT Occupied</h3>\"): tilePCTOccupiedPlots}, \\\n",
    "                {HTML(\"<h3>Intensity Cycle 1</h3>\"): tileIntensityC1Plots}\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_intensity_plots(extDF: DataFrame) -> list:\n",
    "    try:\n",
    "        chart_list = list()\n",
    "        extDF = extDF.na.fill('0')\n",
    "        extDF = extDF.na.replace('nan', '0')\n",
    "        extDF_calc = \\\n",
    "            extDF.\\\n",
    "            selectExpr(\n",
    "                \"cast(Lane as int)\",\n",
    "                \"cast(Cycle as int)\",\n",
    "                *(f\"cast({c} as long)\"\n",
    "                      for c in extDF.columns\n",
    "                          if c.startswith(\"MaxIntensity_\"))).\\\n",
    "            groupBy(\"Lane\", \"Cycle\").\\\n",
    "            agg(\n",
    "                *(sf.mean(c).alias(c)\n",
    "                      for c in extDF.columns\n",
    "                          if c.startswith(\"MaxIntensity_\")))\n",
    "        int_cols = [\n",
    "            c for c in extDF_calc.columns\n",
    "                if c.startswith(\"MaxIntensity_\")]\n",
    "        extDF_calc_pdf = \\\n",
    "            extDF_calc.toPandas()\n",
    "        for c in int_cols:\n",
    "            chart = \\\n",
    "                alt.Chart(extDF_calc_pdf, title=f\"Intensity plots for all lanes for channel {c}\").mark_line(point=True).encode(\n",
    "                    x='Cycle:O',\n",
    "                    y=f'{c}:Q',\n",
    "                    color='Lane:N',\n",
    "                    tooltip=['Lane:N', 'Cycle:O', f'{c}:Q']\n",
    "                ).properties(\n",
    "                    width=900,\n",
    "                    height=400\n",
    "                ).interactive()\n",
    "            chart_list.append(chart)\n",
    "        return chart_list\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_box_plot(tileDF: DataFrame) -> Tuple[alt.Chart, alt.Chart]:\n",
    "    try:\n",
    "        tileDF = tileDF.na.fill('0')\n",
    "        tileDF = tileDF.na.replace('nan', '0')\n",
    "        tileDF_calc = \\\n",
    "            tileDF.\\\n",
    "            selectExpr(\n",
    "                \"cast(Lane as int)\",\n",
    "                \"cast(Tile as int)\",\n",
    "                \"cast(ClusterCount as decimal)\",\n",
    "                \"cast(ClusterCountPF as decimal)\",\n",
    "                \"cast(Density as decimal)\",\n",
    "                \"cast(DensityPF as decimal)\").\\\n",
    "            groupBy(\"Lane\", \"Tile\").\\\n",
    "            agg(\n",
    "                sf.sum(\"ClusterCount\").alias(\"ClusterCount\"),\n",
    "                sf.sum(\"ClusterCountPF\").alias(\"ClusterCountPF\"),\n",
    "                sf.sum(\"Density\").alias(\"Density\"),\n",
    "                sf.sum(\"DensityPF\").alias(\"DensityPF\"),\n",
    "                (sf.sum(\"ClusterCountPF\")/sf.sum(\"ClusterCount\")*100).\\\n",
    "                    alias(\"PCT_ClusterCountPF\"),\n",
    "                (sf.sum(\"DensityPF\")/sf.sum(\"Density\")*100).\\\n",
    "                    alias(\"PCT_DensityPF\"))\n",
    "        tileDF_calc_pdf = \\\n",
    "            tileDF_calc.toPandas()\n",
    "        tileDF_calc_pdf = \\\n",
    "            tileDF_calc_pdf[['Lane', \"Tile\", 'PCT_ClusterCountPF', 'PCT_DensityPF']]\n",
    "        tileDF_calc_pdf['PCT_ClusterCountPF'] = \\\n",
    "            tileDF_calc_pdf['PCT_ClusterCountPF'].astype(float)\n",
    "        tileDF_calc_pdf['PCT_DensityPF'] = \\\n",
    "            tileDF_calc_pdf['PCT_DensityPF'].astype(float)\n",
    "        chart_ClusterCountPF = \\\n",
    "            alt.Chart(tileDF_calc_pdf, title=\"Box plot showing % ClusterCountPF for all lanes\").mark_boxplot().encode(\n",
    "                x='Lane:N',\n",
    "                y=alt.Y('PCT_ClusterCountPF:Q').title('% ClusterCountPF'),\n",
    "                color='Lane:N',\n",
    "                tooltip=['Lane:N', \"Tile:N\", 'PCT_ClusterCountPF:Q']\n",
    "            ).\\\n",
    "            properties(\n",
    "                width=900,\n",
    "                height=400\n",
    "            ).interactive()\n",
    "        chart_DensityPF = \\\n",
    "            alt.Chart(tileDF_calc_pdf, title=\"Box plot showing % DensityPF for all lanes\").mark_boxplot().encode(\n",
    "                x='Lane:N',\n",
    "                y=alt.Y('PCT_DensityPF:Q').title(\"% DensityPF\"),\n",
    "                color='Lane:N',\n",
    "                tooltip=['Lane:N', \"Tile:N\", 'PCT_DensityPF:Q']\n",
    "            ).properties(\n",
    "                width=900,\n",
    "                height=400\n",
    "            ).interactive()\n",
    "        return chart_ClusterCountPF, chart_DensityPF\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pct_occupied_vs_pct_pf(tileDF: DataFrame, extTileDF: DataFrame) -> alt.Chart:\n",
    "    try:\n",
    "        tiles_pct_df = \\\n",
    "            get_occupied_pct_per_tile(\n",
    "                tileDF=tileDF,\n",
    "                extTileDF=extTileDF)\n",
    "        tiles_pct_df_pdf = tiles_pct_df.toPandas()\n",
    "        tiles_pct_df_pdf['PCT_Occupied'] = \\\n",
    "            tiles_pct_df_pdf['PCT_Occupied'].astype(float)\n",
    "        tiles_pct_df_pdf['PCT_ClusterCountPF'] = \\\n",
    "            tiles_pct_df_pdf['PCT_ClusterCountPF'].astype(float)\n",
    "        options = \\\n",
    "            tiles_pct_df_pdf['Lane'].\\\n",
    "            drop_duplicates().\\\n",
    "            values.tolist()\n",
    "        selection = \\\n",
    "            alt.selection_multi(fields=['Lane'])\n",
    "        color = \\\n",
    "            alt.condition(\n",
    "                selection,\n",
    "                alt.Color('Lane:N'),\n",
    "                alt.value('lightgray'))\n",
    "        make_selector = \\\n",
    "            alt.Chart(tiles_pct_df_pdf).\\\n",
    "                mark_rect().\\\n",
    "                encode(x=alt.X('Lane:N').title('Lane'), color=color).\\\n",
    "                add_params(selection)\n",
    "        point = \\\n",
    "            alt.Chart(tiles_pct_df_pdf, title=\"% Occupied vs % PF for all lanes\").mark_point().encode(\n",
    "                x=alt.X('PCT_Occupied:Q').title('% Occupied'),\n",
    "                y=alt.Y('PCT_ClusterCountPF:Q').title('% PF'),\n",
    "                color=alt.Color('Lane:N').scale(domain=options).title('lane'),\n",
    "                tooltip=['Lane:N', \"Tile:N\", 'PCT_Occupied:Q', 'PCT_ClusterCountPF:Q']).\\\n",
    "            add_params(\n",
    "                selection\n",
    "            ).transform_filter(\n",
    "                selection\n",
    "            ).\\\n",
    "            properties(\n",
    "                width=960, height=400)\n",
    "        plot = alt.vconcat(make_selector, point)\n",
    "        return plot\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_plots(\n",
    "        mergedTileDF: DataFrame,\n",
    "        extDF: DataFrame,\n",
    "        tileDF: DataFrame,\n",
    "        extTileDF: DataFrame) -> dict:\n",
    "    try:\n",
    "        all_plots = dict()\n",
    "        ## get pca plots\n",
    "        mergedDF_pdf = mergedTileDF.toPandas()\n",
    "        pca_plot1, pca_plot2 = \\\n",
    "            generatePCA_for_tiles(df=mergedDF_pdf)\n",
    "        all_plots.update(\n",
    "            {'pca': [pca_plot1, pca_plot2]})\n",
    "        ## get tile plots\n",
    "        tilePCTClusterCountPFPlots, tilePCTDensityPFPlots, tilePCTQ30Plots, tilePCTOccupiedPlots, tileIntensityC1Plots = \\\n",
    "            get_tile_plots(df=mergedDF_pdf)\n",
    "        all_plots.update(\n",
    "            {'tile': [tilePCTClusterCountPFPlots, tilePCTDensityPFPlots, tilePCTQ30Plots, tilePCTOccupiedPlots, tileIntensityC1Plots]})\n",
    "        ## get intensity plots\n",
    "        chart_list = \\\n",
    "            generate_intensity_plots(\n",
    "                extDF=extDF)\n",
    "        all_plots.update(\n",
    "            {'intensity': chart_list})\n",
    "        ## box plot\n",
    "        chart_ClusterCountPF, chart_DensityPF = \\\n",
    "            generate_box_plot(tileDF=tileDF)\n",
    "        all_plots.update(\n",
    "            {'boxplot': [chart_ClusterCountPF, chart_DensityPF]})\n",
    "        ## occupied vs pf\n",
    "        occu_plot = \\\n",
    "            plot_pct_occupied_vs_pct_pf(\n",
    "                tileDF=tileDF,\n",
    "                extTileDF=extTileDF)\n",
    "        all_plots.update(\n",
    "            {'occupied': [occu_plot,]})\n",
    "        return all_plots\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_data, rg_qc, lane_qc, df_dict = \\\n",
    "    generate_table_report(\n",
    "        metrics_list=metrics_list,\n",
    "        runinfo_xml=runinfo_xml,\n",
    "        runparameters_xml=runparameters_xml)\n",
    "\n",
    "mergedDF = \\\n",
    "    read_interop_metrics_and_merge_per_tile(\n",
    "        df_dict=df_dict,\n",
    "        runinfo_xml=runinfo_xml)\n",
    "\n",
    "all_plots = \\\n",
    "    get_all_plots(\n",
    "        mergedTileDF=mergedDF,\n",
    "        extDF=df_dict.get(\"Extraction\"),\n",
    "        tileDF=df_dict.get(\"Tile\"),\n",
    "        extTileDF=df_dict.get(\"ExtendedTile\"))\n",
    "\n",
    "## read runinfo file and get reads_stat\n",
    "run_id, flowcell_id, instrument_id, lane_count, reads_stat, _ = \\\n",
    "    read_runinfo_xml(runinfo_xml)\n",
    "## save data\n",
    "## overview_data: save as csv\n",
    "pd.DataFrame(overview_data).\\\n",
    "    to_csv(\n",
    "        overview_csv_output,\n",
    "        index=False)\n",
    "## mergedDF: save as parquet\n",
    "mergedDF.\\\n",
    "    withColumn(\"Run_id\", lit(run_id)).\\\n",
    "    write.\\\n",
    "    parquet(\n",
    "        tile_parquet_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"<ul>\n",
    "    <li><b>Run id:</b> {run_id}</li>\n",
    "    <li><b>Lane counts:</b> {lane_count}</li>\n",
    "    <li><b>Run Cycles:</b></li>\n",
    "</ul>\"\"\"))\n",
    "display(HTML(reads_stat.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Run overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(pd.DataFrame(overview_data).to_html(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run stats for each read groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(pd.DataFrame(rg_qc).to_html(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run stats for each lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lane_id, l_data in lane_qc.items():\n",
    "    display(HTML(f\"<h2>Lane: {lane_id}</h2>\"))\n",
    "    display(HTML(pd.DataFrame(l_data).to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "### Tile plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in all_plots.get(\"tile\"):\n",
    "    for title, plots in p.items():\n",
    "        display(title)\n",
    "        for i in plots:\n",
    "            display(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in all_plots.get(\"intensity\"):\n",
    "    display(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots for % ClusterCountPF and % DensityPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in all_plots.get(\"boxplot\"):\n",
    "    display(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in all_plots.get(\"pca\"):\n",
    "    display(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot for % Occupied vs % PF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in all_plots.get(\"occupied\"):\n",
    "    display(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysam",
   "language": "python",
   "name": "dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
