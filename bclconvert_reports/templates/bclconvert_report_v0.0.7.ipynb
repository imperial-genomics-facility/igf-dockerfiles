{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a5a6ddc-a739-4133-8d12-c4f85f972156",
   "metadata": {},
   "source": [
    "# BCLConvert De-multiplexing Report \n",
    "\n",
    "* __Notebook version__: `v0.0.7`\n",
    "* __Created by:__ `Imperial BRC Genomics Facility`\n",
    "* __Maintained by:__ `Imperial BRC Genomics Facility`\n",
    "* __Docker image:__ `imperialgenomicsfacility/igf-dockerfiles/bclconvert_reports:v5`\n",
    "* __Github repository:__ [imperial-genomics-facility/igf-dockerfiles/bclconvert_reports](https://github.com/imperial-genomics-facility/igf-dockerfiles)\n",
    "* __Contact us:__ [Imperial BRC Genomics Facility](https://www.imperial.ac.uk/medicine/research-and-impact/facilities/genomics-facility/contact-us/)\n",
    "* __License:__ [Apache License 2.0](https://github.com/imperial-genomics-facility/interop-notebook-image/blob/main/LICENSE)\n",
    "* __Created on:__ {{ DATE_TAG }}\n",
    "* __Sequencing run id:__ {{ SEQRUN_IGF_ID }}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3e0f4-b78a-40ca-939d-74b54b47f8d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Load library and generate plots\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from iplotter import ChartJSPlotter\n",
    "from iplotter import GCPlotter\n",
    "from shutil import copy2\n",
    "from shutil import copytree\n",
    "import warnings\n",
    "from IPython.display import HTML\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def combine_bclconvert_demultiplex_stats_csv(demultiplex_stats_list: list) \\\n",
    "        -> pd.DataFrame:\n",
    "    try:\n",
    "        merged_df = pd.DataFrame()\n",
    "        for entry in demultiplex_stats_list:\n",
    "            if not os.path.exists(entry):\n",
    "                raise IOError('Missing file {0}'.format(entry))\n",
    "            df = pd.read_csv(entry)\n",
    "            df['Index'].fillna('', inplace=True)\n",
    "            if len(merged_df.index) == 0:\n",
    "                merged_df = df.copy()\n",
    "            else:\n",
    "                merged_df = \\\n",
    "                    pd.concat(\n",
    "                        [merged_df, df],\n",
    "                        ignore_index=True)\n",
    "        expected_columns = [\n",
    "            'Lane', 'SampleID', 'Index', '# Reads',\n",
    "            '# Perfect Index Reads', '# One Mismatch Index Reads',\n",
    "            '# Two Mismatch Index Reads', '% Reads',\n",
    "            '% Perfect Index Reads', '% One Mismatch Index Reads',\n",
    "            '% Two Mismatch Index Reads']\n",
    "        for i in expected_columns:\n",
    "            if i not in merged_df.columns:\n",
    "                raise KeyError(\n",
    "                    \"Missing column {0} in demultiplex_stats\".\\\n",
    "                    format(i))\n",
    "        combined_df = \\\n",
    "            merged_df.\\\n",
    "            groupby([\n",
    "                'Lane', 'SampleID', 'Index']).\\\n",
    "            agg({\n",
    "                '# Reads': np.sum,\n",
    "                '# Perfect Index Reads': np.sum,\n",
    "                '# One Mismatch Index Reads': np.sum,\n",
    "                '# Two Mismatch Index Reads': np.sum,\n",
    "                '% Reads': np.mean,\n",
    "                '% Perfect Index Reads': np.mean,\n",
    "                '% One Mismatch Index Reads': np.mean,\n",
    "                '% Two Mismatch Index Reads': np.mean})\n",
    "        combined_df.reset_index(inplace=True)\n",
    "        return combined_df\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "                'Failed to combine Demultiplex_Stats csv files, error: {0}'.\\\n",
    "                format(e))\n",
    "\n",
    "def combine_bclconvert_quality_metrics_csv(quality_metrics_list: list) \\\n",
    "        -> pd.DataFrame:\n",
    "    try:\n",
    "        merged_df = pd.DataFrame()\n",
    "        for entry in quality_metrics_list:\n",
    "            if not os.path.exists(entry):\n",
    "                raise IOError('Missing file {0}'.format(entry))\n",
    "            df = pd.read_csv(entry)\n",
    "            if len(merged_df.index) == 0:\n",
    "                merged_df = df.copy()\n",
    "            else:\n",
    "                merged_df = \\\n",
    "                    pd.concat(\n",
    "                        [merged_df, df],\n",
    "                        ignore_index=True)\n",
    "        expected_columns = [\n",
    "            \"Lane\", \"SampleID\", \"index\", \"index2\", \"ReadNumber\",\n",
    "            \"Yield\", \"YieldQ30\", \"QualityScoreSum\", \"% Q30\"]\n",
    "        for i in expected_columns:\n",
    "            if i not in merged_df.columns:\n",
    "                raise KeyError(\n",
    "                    \"Missing column {0} in quality_metrics\".\\\n",
    "                    format(i))\n",
    "        merged_df['index2'] = \\\n",
    "            merged_df['index2'].fillna('')\n",
    "        combined_df = \\\n",
    "            merged_df.\\\n",
    "            groupby([\n",
    "                \"Lane\", \"SampleID\", \"index\", \"index2\", \"ReadNumber\"]).\\\n",
    "            agg({\n",
    "                \"Yield\": np.sum,\n",
    "                \"YieldQ30\": np.sum,\n",
    "                \"QualityScoreSum\": np.sum,\n",
    "                \"Mean Quality Score (PF)\": np.mean,\n",
    "                \"% Q30\": np.mean})\n",
    "        combined_df.reset_index(inplace=True)\n",
    "        return combined_df\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "                'Failed to combine Quality_Metrics csv files, error: {0}'.\\\n",
    "                format(e))\n",
    "\n",
    "def combine_bclconvert_top_unknown_barcodes_csv(\n",
    "        top_unknown_barcodes_list: list) \\\n",
    "        -> pd.DataFrame:\n",
    "    try:\n",
    "        merged_df = pd.DataFrame()\n",
    "        for entry in top_unknown_barcodes_list:\n",
    "            if not os.path.exists(entry):\n",
    "                raise IOError('Missing file {0}'.format(entry))\n",
    "            df = pd.read_csv(entry)\n",
    "            if len(merged_df.index) == 0:\n",
    "                merged_df = df.copy()\n",
    "            else:\n",
    "                merged_df = \\\n",
    "                    pd.concat(\n",
    "                        [merged_df, df],\n",
    "                        ignore_index=True)\n",
    "        expected_columns = [\n",
    "            'Lane', 'index', 'index2', '# Reads',\n",
    "            '% of Unknown Barcodes', '% of All Reads']\n",
    "        for i in expected_columns:\n",
    "            if i not in merged_df.columns:\n",
    "                raise KeyError(\n",
    "                    \"Missing column {0} in quality_metrics\".\\\n",
    "                    format(i))\n",
    "        merged_df['index2'] = \\\n",
    "            merged_df['index2'].fillna('')\n",
    "        combined_df = \\\n",
    "            merged_df.\\\n",
    "            groupby([\n",
    "                'Lane', 'index', 'index2']).\\\n",
    "            agg({\n",
    "                '# Reads': np.sum,\n",
    "                '% of Unknown Barcodes': np.mean,\n",
    "                '% of All Reads': np.mean})\n",
    "        combined_df.reset_index(inplace=True)\n",
    "        combined_df = \\\n",
    "            combined_df.\\\n",
    "            sort_values('# Reads', ascending=False).\\\n",
    "            head(200).\\\n",
    "            sort_values([\n",
    "                'Lane', '# Reads'], \n",
    "                ascending=False)\n",
    "        return combined_df\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "                'Failed to combine Top_Unknown_Barcodes csv files, error: {0}'.\\\n",
    "                format(e))\n",
    "\n",
    "def combine_bclconvert_index_hopping_counts_csv(\n",
    "        index_hopping_counts_list: list) \\\n",
    "        -> pd.DataFrame:\n",
    "    try:\n",
    "        merged_df = pd.DataFrame()\n",
    "        for entry in index_hopping_counts_list:\n",
    "            if not os.path.exists(entry):\n",
    "                raise IOError('Missing file {0}'.format(entry))\n",
    "            df = pd.read_csv(entry)\n",
    "            if len(merged_df.index) == 0:\n",
    "                merged_df = df.copy()\n",
    "            else:\n",
    "                merged_df = \\\n",
    "                    pd.concat(\n",
    "                        [merged_df, df],\n",
    "                        ignore_index=True)\n",
    "        expected_columns = [\n",
    "            'Lane', 'SampleID', 'index', 'index2', '# Reads',\n",
    "            '% of Hopped Reads', '% of All Reads']\n",
    "        for i in expected_columns:\n",
    "            if i not in merged_df.columns:\n",
    "                raise KeyError(\n",
    "                    \"Missing column {0} in quality_metrics\".\\\n",
    "                    format(i))\n",
    "        merged_df['SampleID'] = \\\n",
    "            merged_df['SampleID'].fillna('UNKNOWN')\n",
    "        merged_df['Sample_Project'] = \\\n",
    "            merged_df['Sample_Project'].fillna('UNKNOWN')\n",
    "        merged_df['% of Hopped Reads'] = \\\n",
    "            merged_df['% of Hopped Reads'].fillna(0)\n",
    "        merged_df.dropna(inplace=True)\n",
    "        filt_merged_df = \\\n",
    "            merged_df[(merged_df['% of Hopped Reads'] > 0.005) & (merged_df['SampleID'] != 'UNKNOWN')]\n",
    "        combined_df = \\\n",
    "            filt_merged_df.\\\n",
    "            groupby([\n",
    "                'Lane', 'SampleID', 'index', 'index2']).\\\n",
    "            agg({\n",
    "                '# Reads': np.sum,\n",
    "                '% of Hopped Reads': np.mean,\n",
    "                '% of All Reads': np.mean})\n",
    "        combined_df.reset_index(inplace=True)\n",
    "        return combined_df\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "                'Failed to combine Index_Hopping_Counts csv files, error: {0}'.\\\n",
    "                format(e))\n",
    "\n",
    "def get_samplesheet_records(samplesheets: list) \\\n",
    "        -> pd.DataFrame:\n",
    "    try:\n",
    "        all_samplesheet_data = pd.DataFrame()\n",
    "        for f in samplesheets:\n",
    "            data_section = False\n",
    "            samplesheet_data_list = list()\n",
    "            with open(f, 'r') as fp:\n",
    "                for i in fp:\n",
    "                    i = i.strip()\n",
    "                    if i == '':\n",
    "                        continue\n",
    "                    if i.startswith('['):\n",
    "                        data_section = False\n",
    "                        if i.startswith('[Data]') or \\\n",
    "                           i.startswith('[data]') or \\\n",
    "                           i.startswith('[BCLConvert_Data]'):\n",
    "                            data_section = True\n",
    "                            continue\n",
    "                    if data_section:\n",
    "                        samplesheet_data_list.\\\n",
    "                            append(i.split(','))\n",
    "                samplesheet = \\\n",
    "                    pd.DataFrame(\n",
    "                        samplesheet_data_list[1:],\n",
    "                        columns=samplesheet_data_list[0])\n",
    "                all_samplesheet_data = \\\n",
    "                    pd.concat(\n",
    "                        [all_samplesheet_data, samplesheet],\n",
    "                        ignore_index=True)\n",
    "        return all_samplesheet_data\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)\n",
    "\n",
    "def get_interop_index_stats(\n",
    "      run_dir: str,\n",
    "      reports_dir: str,\n",
    "      interop_dir_name: str = 'InterOp',\n",
    "      index_metrix_file: str = 'IndexMetricsOut.bin') \\\n",
    "        -> pd.DataFrame:\n",
    "    try:\n",
    "        flowcell_summary_data = list()\n",
    "        index_metrix_file_path = \\\n",
    "            os.path.join(run_dir, interop_dir_name, index_metrix_file)\n",
    "        if os.path.exists(index_metrix_file_path):\n",
    "            index_csv = \\\n",
    "                subprocess.\\\n",
    "                check_output([\n",
    "                    \"/opt/interop/bin/index-summary\",\n",
    "                    run_dir,\n",
    "                    \"--csv=1\"])\n",
    "        else:\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                source_interop_path = \\\n",
    "                    os.path.join(run_dir, interop_dir_name)\n",
    "                target_interop_dir = \\\n",
    "                    os.path.join(temp_dir, interop_dir_name)\n",
    "                copytree(\n",
    "                    source_interop_path,\n",
    "                    target_interop_dir)\n",
    "                for i in os.listdir(run_dir):\n",
    "                    if i.endswith(\".xml\"):\n",
    "                        source_path = \\\n",
    "                            os.path.join(run_dir, i)\n",
    "                        target_path = \\\n",
    "                            os.path.join(temp_dir, i)\n",
    "                        copy2(source_path, target_path)\n",
    "                source_index_file = \\\n",
    "                    os.path.join(reports_dir, index_metrix_file)\n",
    "                target_index_file = \\\n",
    "                    os.path.join(temp_dir, interop_dir_name, index_metrix_file)\n",
    "                copy2(source_index_file, target_index_file)\n",
    "                index_csv = \\\n",
    "                    subprocess.\\\n",
    "                    check_output([\n",
    "                        \"/opt/interop/bin/index-summary\",\n",
    "                        temp_dir,\n",
    "                        \"--csv=1\"])\n",
    "        if isinstance(index_csv, bytes):\n",
    "            index_csv = index_csv.decode('utf-8')\n",
    "        index_csv = index_csv.split(\"\\n\")\n",
    "        counter = 99\n",
    "        key = None\n",
    "        lane_data = list()\n",
    "        total_reads = dict()\n",
    "        for i in index_csv:\n",
    "            if i.startswith('Lane'):\n",
    "                counter = 0\n",
    "                key = i.split(\",\")[0]\n",
    "                if key is not None:\n",
    "                    key = key.replace(\"Lane\", \"\").strip()\n",
    "                lane_data = list()\n",
    "                continue\n",
    "            if counter < 2:\n",
    "                lane_data.append(i)\n",
    "                counter += 1\n",
    "            if counter == 2:\n",
    "                total_reads.update({key: lane_data})\n",
    "        formatted_lane_data = dict()\n",
    "        for lane_id, lane_data in total_reads.items():\n",
    "            csv_data = StringIO('\\n'.join(total_reads.get(lane_id)))\n",
    "            data = pd.read_csv(csv_data).to_dict(orient='records')\n",
    "            formatted_lane_data.update({lane_id: data})\n",
    "        for lane_id, lane_data in formatted_lane_data.items():\n",
    "            flowcell_summary_data.\\\n",
    "                append({\n",
    "                    'Lane': lane_id,\n",
    "                    'Total Reads': lane_data[0].get('Total Reads'),\n",
    "                    'PF Reads': lane_data[0].get('PF Reads')})\n",
    "        flowcell_summary_data = \\\n",
    "            pd.DataFrame(flowcell_summary_data)\n",
    "        return flowcell_summary_data\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)\n",
    "\n",
    "def merge_known_samples(\n",
    "    demultiplex_stats_df: pd.DataFrame,\n",
    "    quality_metrics_df: pd.DataFrame,\n",
    "    samplesheet_df: pd.DataFrame) \\\n",
    "        -> pd.DataFrame:\n",
    "    try:\n",
    "        temp_demultiplex_stats_df = \\\n",
    "            demultiplex_stats_df.\\\n",
    "            copy()\n",
    "        temp_quality_metrics_df = \\\n",
    "            quality_metrics_df.\\\n",
    "            copy()\n",
    "        temp_samplesheet_df = \\\n",
    "            samplesheet_df.\\\n",
    "            copy()\n",
    "        filt_quality_metrics_df = \\\n",
    "            temp_quality_metrics_df[\n",
    "                ~temp_quality_metrics_df['ReadNumber'].\n",
    "                str.startswith(\"I\")]\n",
    "        if 'index2' in filt_quality_metrics_df.columns:\n",
    "            filt_quality_metrics_df['Index'] = \\\n",
    "                filt_quality_metrics_df[['index', 'index2']].\\\n",
    "                agg('-'.join, axis=1)\n",
    "        else:\n",
    "            filt_quality_metrics_df['Index'] = \\\n",
    "                filt_quality_metrics_df['index'].copy()\n",
    "        agg_combined_qmetrics_df = \\\n",
    "            filt_quality_metrics_df.\\\n",
    "            groupby(['Lane', 'SampleID', 'Index']).\\\n",
    "            agg({\n",
    "                'Yield': 'sum',\n",
    "                'Mean Quality Score (PF)': 'mean',\n",
    "                '% Q30': 'mean'})\n",
    "        joined_data1 = \\\n",
    "            temp_demultiplex_stats_df.\\\n",
    "            set_index(['Lane', 'SampleID', 'Index']).\\\n",
    "            join(agg_combined_qmetrics_df, how='left').\\\n",
    "            reset_index()\n",
    "        joined_data1 = \\\n",
    "            joined_data1[[\n",
    "                'Lane',\n",
    "                'SampleID',\n",
    "                'Index',\n",
    "                '# Reads',\n",
    "                '% Reads',\n",
    "                '% Perfect Index Reads',\n",
    "                'Yield',\n",
    "                '% Q30',\n",
    "                'Mean Quality Score (PF)']]\n",
    "        joined_data1.\\\n",
    "            set_index('SampleID', inplace=True)\n",
    "        temp_samplesheet_df.\\\n",
    "            set_index('Sample_ID', inplace=True)\n",
    "        final_joined_data = \\\n",
    "            joined_data1.\\\n",
    "            join(temp_samplesheet_df[['Sample_Name', 'Sample_Project']].\\\n",
    "                 drop_duplicates(),\n",
    "                 how='left')\n",
    "        final_joined_data.index.rename('SampleID', inplace=True)\n",
    "        final_joined_data.reset_index(inplace=True)\n",
    "        final_joined_data = \\\n",
    "            final_joined_data[[\n",
    "                'Lane',\n",
    "                'Sample_Project',\n",
    "                'SampleID',\n",
    "                'Sample_Name',\n",
    "                'Index',\n",
    "                '# Reads',\n",
    "                '% Reads',\n",
    "                '% Perfect Index Reads',\n",
    "                'Yield',\n",
    "                '% Q30',\n",
    "                'Mean Quality Score (PF)']]\n",
    "        final_joined_data['Sample_Project'].\\\n",
    "            fillna('UNKNOWN', inplace=True)\n",
    "        final_joined_data['Sample_Name'].\\\n",
    "            fillna('UNKNOWN', inplace=True)\n",
    "        final_joined_data.\\\n",
    "            sort_values([\n",
    "                'Lane',\n",
    "                'Sample_Project',\n",
    "                '# Reads'],\n",
    "                ascending=[True, True, False],\n",
    "                inplace=True)\n",
    "        return final_joined_data\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)\n",
    "\n",
    "def get_flowcell_summary_plot(flowcell_summary_df: pd.DataFrame) -> dict:\n",
    "    try:\n",
    "        temp_flowcell_summary_df = \\\n",
    "            flowcell_summary_df.\\\n",
    "            copy()\n",
    "        labels = \\\n",
    "            temp_flowcell_summary_df['Lane'].\\\n",
    "            map(lambda x: 'Lane {0}'.format(x)).\\\n",
    "            values.tolist()\n",
    "        datasets = [{\n",
    "            \"label\": \"Total cluster raw\",\n",
    "            \"data\": temp_flowcell_summary_df[\"Total Reads\"].astype(int).values.tolist(),\n",
    "            \"backgroundColor\": 'rgba(255, 99, 132, 0.8)',\n",
    "            \"borderColor\": 'rgba(255, 99, 132, 0.8)',\n",
    "            \"borderWidth\": 1},{\n",
    "            \"label\": \"Total cluster pf\",\n",
    "            \"data\": temp_flowcell_summary_df[\"PF Reads\"].astype(int).values.tolist(),\n",
    "            \"backgroundColor\": 'rgba(54, 162, 235, 0.8)',\n",
    "            \"borderColor\": 'rgba(54, 162, 235, 0.8)',\n",
    "            \"borderWidth\": 1}]\n",
    "        options = {\n",
    "            \"scales\": {\n",
    "                \"y\": {\n",
    "                    \"beginAtZero\": \"true\"\n",
    "                }\n",
    "            },\n",
    "            \"responsive\": \"true\",\n",
    "            \"title\": {\n",
    "                \"display\": \"true\",\n",
    "                \"text\": 'Raw vs PF cluster counts per lane',\n",
    "                \"position\": \"bottom\"\n",
    "            },\n",
    "            \"plugins\": {\n",
    "                \"legend\": {\n",
    "                    \"position\": 'top',\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        cj_plotter = ChartJSPlotter()\n",
    "        data = {\n",
    "            \"labels\": labels,\n",
    "            \"datasets\": datasets\n",
    "        }\n",
    "        plot = \\\n",
    "            cj_plotter.\\\n",
    "            plot(\n",
    "                data,\n",
    "                'bar',\n",
    "                options=options,\n",
    "                w=400,\n",
    "                h=300)\n",
    "        return plot\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)\n",
    "\n",
    "def get_flowcell_project_summary_plot(merged_data: pd.DataFrame) \\\n",
    "        -> dict:\n",
    "    try:\n",
    "        temp_merged_data = \\\n",
    "            merged_data.\\\n",
    "            copy()\n",
    "        project_groups = \\\n",
    "            temp_merged_data.\\\n",
    "            groupby(['Lane', 'Sample_Project']).\\\n",
    "            agg({'# Reads': 'sum'}).\\\n",
    "            reset_index()\n",
    "        project_group_df = list()\n",
    "        for lane_id, l_data in project_groups.groupby('Lane'):\n",
    "            data_row = {'Lane': 'Lane {0}'.format(lane_id)}\n",
    "            for project_id, p_data in l_data.groupby('Sample_Project'):\n",
    "                data_row.update({project_id: p_data['# Reads'].sum()})\n",
    "            project_group_df.append(data_row)\n",
    "        project_group_df = \\\n",
    "            pd.DataFrame(project_group_df)\n",
    "        data = list()\n",
    "        data.append(project_group_df.columns.tolist())\n",
    "        data.extend(project_group_df.values.tolist())\n",
    "        options = {\n",
    "            \"title\": 'Project summary plot',\n",
    "            \"width\": 600,\n",
    "            \"height\": 400,\n",
    "            \"chartArea\": {\"left\": 50, \"width\": \"60%\"},\n",
    "            \"legend\": {\"position\": 'right', \"maxLines\": 20, \"fontSize\": 3},\n",
    "            \"dataOpacity\": 0.5,\n",
    "            \"colors\": [\n",
    "                '#0173B2', '#DE8F05', '#029E73', '#D55E00', '#CC78BC', '#CA9161',\n",
    "                '#FBAFE4', '#949494', '#ECE133', '#56B4E9', '#0173B2'],\n",
    "            \"bar\": {\"groupWidth\": '70%'},\n",
    "            \"isStacked\": \"percent\"}\n",
    "        gcplotter = GCPlotter()\n",
    "        plot = \\\n",
    "            gcplotter.\\\n",
    "            plot(\n",
    "                data,\n",
    "                chart_type=\"BarChart\",\n",
    "                chart_package='corechart',\n",
    "                options=options)\n",
    "        return plot\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)\n",
    "\n",
    "def get_flowcell_project_summary_table(merged_data: pd.DataFrame) \\\n",
    "        -> pd.DataFrame:\n",
    "    try:\n",
    "        temp_merged_data = \\\n",
    "            merged_data.\\\n",
    "            copy()\n",
    "        if 'Lane' in temp_merged_data.columns:\n",
    "            summary_df = \\\n",
    "                temp_merged_data.\\\n",
    "                groupby(['Lane', 'Sample_Project']).\\\n",
    "                agg(len)['SampleID'].\\\n",
    "                reset_index()\n",
    "        else:\n",
    "            summary_df = \\\n",
    "                temp_merged_data.\\\n",
    "                groupby(['Sample_Project',]).\\\n",
    "                agg(len)['SampleID'].\\\n",
    "                reset_index()\n",
    "        return summary_df\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)\n",
    "\n",
    "def get_sample_dist_plots(\n",
    "        merged_data: pd.DataFrame,\n",
    "        get_plots: bool = True) \\\n",
    "        -> dict:\n",
    "    try:\n",
    "        temp_merged_data = \\\n",
    "            merged_data.\\\n",
    "            copy()\n",
    "        lane_plots = dict()\n",
    "        cj_plotter = ChartJSPlotter()\n",
    "        bg_colors = [\n",
    "            'rgba(255, 99, 132, 0.8)',\n",
    "            'rgba(54, 162, 235, 0.8)',\n",
    "            'rgba(255, 206, 86, 0.8)',\n",
    "            'rgba(75, 192, 192, 0.8)',\n",
    "            'rgba(153, 102, 255, 0.8)',\n",
    "            'rgba(255, 159, 64, 0.8)',\n",
    "            'rgba(255, 159, 10, 0.8)',\n",
    "            'rgba(255, 159, 192, 0.8)']\n",
    "        border_colors = [\n",
    "            'rgba(255, 99, 132, 0.8)',\n",
    "            'rgba(54, 162, 235, 0.8)',\n",
    "            'rgba(255, 206, 86, 0.8)',\n",
    "            'rgba(75, 192, 192, 0.8)',\n",
    "            'rgba(153, 102, 255, 0.8)',\n",
    "            'rgba(255, 159, 64, 0.8)',\n",
    "            'rgba(255, 159, 10, 0.8)',\n",
    "            'rgba(255, 159, 192, 0.8)']\n",
    "        options = {\n",
    "            'scales': {\n",
    "                'y': {\n",
    "                    'beginAtZero': True\n",
    "                },\n",
    "                'xAxes': [{\n",
    "                    'ticks': {\n",
    "                        'fontSize': 8\n",
    "                    }\n",
    "                }]\n",
    "            },\n",
    "            'responsive': True,\n",
    "            'plugins': {\n",
    "                'legend': {\n",
    "                    'position': 'top'}\n",
    "            }}\n",
    "        if 'Lane' in temp_merged_data.columns:\n",
    "            for lane_id, l_data in temp_merged_data.groupby('Lane'):\n",
    "                lane_samples = l_data['SampleID'].values.tolist()\n",
    "                datasets = list()\n",
    "                counter = 0\n",
    "                for project_name, p_data in l_data.groupby('Sample_Project'):\n",
    "                    read_counts = \\\n",
    "                        p_data.\\\n",
    "                        set_index('SampleID')['# Reads'].\\\n",
    "                        reindex(lane_samples).\\\n",
    "                        fillna(0).\\\n",
    "                        values.\\\n",
    "                        tolist()\n",
    "                    datasets.append({\n",
    "                        \"label\": project_name,\n",
    "                        \"data\": read_counts,\n",
    "                        \"backgroundColor\": bg_colors[counter],\n",
    "                        \"borderColor\": border_colors[counter]})\n",
    "                    counter += 1\n",
    "                data = {\n",
    "                    \"labels\": lane_samples,\n",
    "                    \"datasets\": datasets}\n",
    "                if get_plots:\n",
    "                    plot = \\\n",
    "                        cj_plotter.\\\n",
    "                        plot(\n",
    "                            data,\n",
    "                            'bar',\n",
    "                            options=options,\n",
    "                            w=800,\n",
    "                            h=400)\n",
    "                    lane_plots.update({int(lane_id): plot})\n",
    "                else:\n",
    "                    lane_plots.update({int(lane_id): data})\n",
    "        else:\n",
    "            lane_samples = temp_merged_data['SampleID'].values.tolist()\n",
    "            datasets = list()\n",
    "            counter = 0\n",
    "            for project_name, p_data in temp_merged_data.groupby('Sample_Project'):\n",
    "                read_counts = \\\n",
    "                    p_data.\\\n",
    "                    set_index('SampleID')['# Reads'].\\\n",
    "                    reindex(lane_samples).\\\n",
    "                    fillna(0).\\\n",
    "                    values.\\\n",
    "                    tolist()\n",
    "                datasets.append({\n",
    "                    \"label\": project_name,\n",
    "                    \"data\": read_counts,\n",
    "                    \"backgroundColor\": bg_colors[counter],\n",
    "                    \"borderColor\": border_colors[counter]})\n",
    "                counter += 1\n",
    "            data = {\n",
    "                \"labels\": lane_samples,\n",
    "                \"datasets\": datasets}\n",
    "            if get_plots:\n",
    "                plot = \\\n",
    "                    cj_plotter.\\\n",
    "                    plot(\n",
    "                        data,\n",
    "                        'bar',\n",
    "                        options=options,\n",
    "                        w=800,\n",
    "                        h=400)\n",
    "                lane_plots.update({1: plot})\n",
    "            else:\n",
    "                lane_plots.update({1: data})\n",
    "        return lane_plots\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)\n",
    "\n",
    "def get_undetermined_table(\n",
    "        unknown_df: pd.DataFrame) \\\n",
    "        -> pd.DataFrame:\n",
    "    try:\n",
    "        temp_unknown_df = \\\n",
    "            unknown_df.\\\n",
    "            copy()\n",
    "        temp_unknown_df['index_RC'] = \\\n",
    "            temp_unknown_df['index'].\\\n",
    "                map(lambda x: x.translate(str.maketrans('ATGC', 'TACG'))[::-1])\n",
    "        if 'index2' in temp_unknown_df.columns:\n",
    "            temp_unknown_df['index2_RC'] = \\\n",
    "            temp_unknown_df['index2'].\\\n",
    "                map(lambda x: x.translate(str.maketrans('ATGC', 'TACG'))[::-1])\n",
    "            temp_unknown_df['Barcode'] = \\\n",
    "                temp_unknown_df[['index', 'index2']].\\\n",
    "                agg('-'.join, axis=1)\n",
    "            temp_unknown_df['Barcode_I1_RC'] = \\\n",
    "                temp_unknown_df[['index_RC', 'index2']].\\\n",
    "                agg('-'.join, axis=1)\n",
    "            temp_unknown_df['Barcode_I2_RC'] = \\\n",
    "                temp_unknown_df[['index', 'index2_RC']].\\\n",
    "                agg('-'.join, axis=1)\n",
    "        else:\n",
    "            temp_unknown_df['Barcode'] = \\\n",
    "                temp_unknown_df['index'].\\\n",
    "                copy()\n",
    "            temp_unknown_df['Barcode'] = \\\n",
    "                temp_unknown_df['index_RC'].\\\n",
    "                copy()\n",
    "            temp_unknown_df['Barcode_I2_RC'] = ''\n",
    "        temp_unknown_df = \\\n",
    "            temp_unknown_df[[\n",
    "                'Lane',\n",
    "                '# Reads',\n",
    "                'Barcode',\n",
    "                'Barcode_I1_RC',\n",
    "                'Barcode_I2_RC']]\n",
    "        temp_unknown_df.\\\n",
    "            sort_values([\n",
    "                'Lane',\n",
    "                '# Reads'],\n",
    "                ascending=[True, False],\n",
    "                inplace=True)\n",
    "        return temp_unknown_df\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)\n",
    "\n",
    "\n",
    "def get_undetermined_plots(\n",
    "        unknown_df: pd.DataFrame,\n",
    "        get_plots: bool = True) \\\n",
    "        -> dict:\n",
    "    try:\n",
    "        bg_colors = [\n",
    "            'rgba(255, 99, 132, 0.8)',\n",
    "            'rgba(54, 162, 235, 0.8)',\n",
    "            'rgba(255, 206, 86, 0.8)',\n",
    "            'rgba(75, 192, 192, 0.8)',\n",
    "            'rgba(153, 102, 255, 0.8)',\n",
    "            'rgba(255, 159, 64, 0.8)',\n",
    "            'rgba(255, 159, 10, 0.8)',\n",
    "            'rgba(255, 159, 192, 0.8)']\n",
    "        border_colors = [\n",
    "            'rgba(255, 99, 132, 0.8)',\n",
    "            'rgba(54, 162, 235, 0.8)',\n",
    "            'rgba(255, 206, 86, 0.8)',\n",
    "            'rgba(75, 192, 192, 0.8)',\n",
    "            'rgba(153, 102, 255, 0.8)',\n",
    "            'rgba(255, 159, 64, 0.8)',\n",
    "            'rgba(255, 159, 10, 0.8)',\n",
    "            'rgba(255, 159, 192, 0.8)']\n",
    "        cj_plotter = ChartJSPlotter()\n",
    "        temp_unknown_df = \\\n",
    "            unknown_df.\\\n",
    "            copy()\n",
    "        if 'index2' in temp_unknown_df.columns:\n",
    "            temp_unknown_df['Index'] = \\\n",
    "                temp_unknown_df[['index', 'index2']].\\\n",
    "                agg('-'.join, axis=1)\n",
    "        else:\n",
    "            temp_unknown_df['Index'] = \\\n",
    "                temp_unknown_df['index'].\\\n",
    "                copy()\n",
    "        undetermined_plots = dict()\n",
    "        options = {\n",
    "            'scales': {\n",
    "                'y': {\n",
    "                    'beginAtZero': True\n",
    "                },\n",
    "                'xAxes': [{\n",
    "                    'ticks': {\n",
    "                        'fontSize': 8\n",
    "                    }\n",
    "                }]\n",
    "            },\n",
    "            'responsive': True,\n",
    "            'plugins': {\n",
    "                'legend': {\n",
    "                    'position': 'top'}\n",
    "            }}\n",
    "        if 'Lane' in temp_unknown_df.columns:\n",
    "            for lane_id, l_data in temp_unknown_df.groupby('Lane'):\n",
    "                barcode_labels = l_data['Index'].values.tolist()\n",
    "                barcode_count = l_data['# Reads'].values.tolist()\n",
    "                data = {\n",
    "                    \"labels\": barcode_labels,\n",
    "                    \"datasets\": [{\n",
    "                        'label': 'Undetermined - Lane {0}'.format(lane_id),\n",
    "                        'data': barcode_count,\n",
    "                        \"backgroundColor\": bg_colors[0],\n",
    "                        \"borderColor\": border_colors[0]}]}\n",
    "                if get_plots:\n",
    "                    plot = \\\n",
    "                        cj_plotter.\\\n",
    "                        plot(\n",
    "                            data,\n",
    "                            'bar',\n",
    "                            options=options,\n",
    "                            w=800,\n",
    "                            h=400)\n",
    "                    undetermined_plots.\\\n",
    "                        update({int(lane_id): plot})\n",
    "                else:\n",
    "                    undetermined_plots.\\\n",
    "                        update({int(lane_id): data})\n",
    "        else:\n",
    "            barcode_labels = temp_unknown_df['Index'].values.tolist()\n",
    "            barcode_count = temp_unknown_df['# Reads'].values.tolist()\n",
    "            data = {\n",
    "                \"labels\": barcode_labels,\n",
    "                \"datasets\": [{\n",
    "                    'label': 'Undetermined - Lane {0}'.format(1),\n",
    "                    'data': barcode_count,\n",
    "                    \"backgroundColor\": bg_colors[0],\n",
    "                    \"borderColor\": border_colors[0]}]}\n",
    "            if get_plots:\n",
    "                plot = \\\n",
    "                    cj_plotter.\\\n",
    "                    plot(\n",
    "                        data,\n",
    "                        'bar',\n",
    "                        options=options,\n",
    "                        w=800,\n",
    "                        h=400)\n",
    "                undetermined_plots.\\\n",
    "                    update({1: plot})\n",
    "            else:\n",
    "                undetermined_plots.\\\n",
    "                    update({1: data})\n",
    "        return undetermined_plots\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)\n",
    "\n",
    "\n",
    "def get_hop_plot(\n",
    "        hop_df: pd.DataFrame,\n",
    "        get_plot: bool = True) \\\n",
    "        -> dict:\n",
    "    try:\n",
    "        lane_data = list()\n",
    "        options = {\n",
    "            'scales': {\n",
    "                'y': {\n",
    "                    'beginAtZero': True\n",
    "                },\n",
    "                'xAxes': [{\n",
    "                    'ticks': {\n",
    "                        'fontSize': 8\n",
    "                    }\n",
    "                }]\n",
    "            },\n",
    "            'responsive': True,\n",
    "            'plugins': {\n",
    "                'legend': {\n",
    "                    'position': 'top'}\n",
    "            }}\n",
    "        bg_colors = [\n",
    "            'rgba(255, 99, 132, 0.8)',\n",
    "            'rgba(54, 162, 235, 0.8)',\n",
    "            'rgba(255, 206, 86, 0.8)',\n",
    "            'rgba(75, 192, 192, 0.8)',\n",
    "            'rgba(153, 102, 255, 0.8)',\n",
    "            'rgba(255, 159, 64, 0.8)',\n",
    "            'rgba(255, 159, 10, 0.8)',\n",
    "            'rgba(255, 159, 192, 0.8)']\n",
    "        border_colors = [\n",
    "            'rgba(255, 99, 132, 0.8)',\n",
    "            'rgba(54, 162, 235, 0.8)',\n",
    "            'rgba(255, 206, 86, 0.8)',\n",
    "            'rgba(75, 192, 192, 0.8)',\n",
    "            'rgba(153, 102, 255, 0.8)',\n",
    "            'rgba(255, 159, 64, 0.8)',\n",
    "            'rgba(255, 159, 10, 0.8)',\n",
    "            'rgba(255, 159, 192, 0.8)']\n",
    "        cj_plotter = ChartJSPlotter()\n",
    "        temp_hop_df = hop_df.copy()\n",
    "        if 'index2' in temp_hop_df:\n",
    "            temp_hop_df['Index'] = \\\n",
    "                temp_hop_df[['index', 'index2']].\\\n",
    "                agg('-'.join, axis=1)\n",
    "        else:\n",
    "            temp_hop_df['Index'] = \\\n",
    "                temp_hop_df['index'].copy()\n",
    "        if 'Lane' in temp_hop_df:\n",
    "            for lane_id, l_data in temp_hop_df.groupby('Lane'):\n",
    "                data_row = {'Lane': lane_id}\n",
    "                for index, i_data in l_data.groupby('Index'):\n",
    "                    data_row.update({index: i_data['# Reads'].sum()})\n",
    "                lane_data.append(data_row)\n",
    "        else:\n",
    "            data_row = {'Lane': 1}\n",
    "            for index, i_data in temp_hop_df.groupby('Index'):\n",
    "                data_row.update({index: i_data['# Reads'].sum()})\n",
    "            lane_data.append(data_row)\n",
    "        lane_data = pd.DataFrame(lane_data)\n",
    "        lane_data.fillna(0, inplace=True)\n",
    "        barcodes = [c for c in lane_data.columns if c != 'Lane']\n",
    "        dataset = list()\n",
    "        counter = 0\n",
    "        for lane_id, l_data in lane_data.groupby('Lane'):\n",
    "            barcode_count = l_data[barcodes].values.tolist()[0]\n",
    "            dataset.append({\n",
    "                'label': 'Hopping - Lane {0}'.format(lane_id),\n",
    "                'data': barcode_count,\n",
    "                \"backgroundColor\": bg_colors[counter],\n",
    "                \"borderColor\": border_colors[counter]})\n",
    "            counter += 1\n",
    "        data = {\n",
    "            'labels': barcodes,\n",
    "            'datasets': dataset}\n",
    "        if get_plot:\n",
    "            data = \\\n",
    "                cj_plotter.\\\n",
    "                plot(\n",
    "                    data,\n",
    "                    'bar',\n",
    "                    options=options,\n",
    "                    w=800,\n",
    "                    h=400)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)\n",
    "\n",
    "def get_demult_report_and_plots_for_bclconvert(\n",
    "        run_dir: str,\n",
    "        reports_dir: str) -> None:\n",
    "    try:\n",
    "        if not os.path.exists(run_dir):\n",
    "            raise IOError('Missing run dir {0}'.format(run_dir))\n",
    "        required_report_csvs = [\n",
    "            'Demultiplex_Stats.csv',\n",
    "            'Quality_Metrics.csv',\n",
    "            'Top_Unknown_Barcodes.csv',\n",
    "            'SampleSheet.csv']\n",
    "        for f in required_report_csvs:\n",
    "            filepath = \\\n",
    "                os.path.join(reports_dir, f)\n",
    "            if not os.path.exists(filepath):\n",
    "                raise IOError('Missing report file {0}'.format(filepath))\n",
    "        combined_demux_df = \\\n",
    "            combine_bclconvert_demultiplex_stats_csv([\n",
    "                os.path.join(reports_dir, 'Demultiplex_Stats.csv')])\n",
    "        combined_qmetrics_df = \\\n",
    "            combine_bclconvert_quality_metrics_csv([\n",
    "                os.path.join(reports_dir, 'Quality_Metrics.csv')])\n",
    "        combined_unknown_df = \\\n",
    "            combine_bclconvert_top_unknown_barcodes_csv([\n",
    "                os.path.join(reports_dir, 'Top_Unknown_Barcodes.csv')])\n",
    "        combined_ihop_df = pd.DataFrame()\n",
    "        hop_plot = {}\n",
    "        if os.path.exists(os.path.join(reports_dir, 'Index_Hopping_Counts.csv')):\n",
    "            combined_ihop_df = \\\n",
    "                combine_bclconvert_index_hopping_counts_csv([\n",
    "                    os.path.join(reports_dir, 'Index_Hopping_Counts.csv')])\n",
    "            if len(combined_ihop_df.index) > 0:\n",
    "                hop_plot = \\\n",
    "                    get_hop_plot(combined_ihop_df)\n",
    "        samplesheet_df = \\\n",
    "            get_samplesheet_records(\n",
    "                samplesheets=[os.path.join(reports_dir, 'SampleSheet.csv')])\n",
    "        flowcell_summary_data = \\\n",
    "            get_interop_index_stats(run_dir=run_dir, reports_dir=reports_dir)\n",
    "        flowcell_summary_data_plot = \\\n",
    "            get_flowcell_summary_plot(flowcell_summary_data)\n",
    "        merged_df = \\\n",
    "            merge_known_samples(\n",
    "                demultiplex_stats_df=combined_demux_df,\n",
    "                quality_metrics_df=combined_qmetrics_df,\n",
    "                samplesheet_df=samplesheet_df)\n",
    "        flowcell_project_summary_plot = \\\n",
    "            get_flowcell_project_summary_plot(merged_df)\n",
    "        flowcell_project_summary_table = \\\n",
    "            get_flowcell_project_summary_table(merged_df)\n",
    "        sample_dist_plots = \\\n",
    "            get_sample_dist_plots(merged_df)\n",
    "        undetermined_plots = \\\n",
    "            get_undetermined_plots(combined_unknown_df)\n",
    "        undetermined_table = \\\n",
    "            get_undetermined_table(combined_unknown_df)\n",
    "        return flowcell_summary_data_plot, flowcell_project_summary_plot, \\\n",
    "            merged_df, flowcell_project_summary_table, sample_dist_plots, \\\n",
    "            undetermined_plots, undetermined_table, combined_ihop_df, \\\n",
    "            hop_plot\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)\n",
    "\n",
    "## wikipedia code\n",
    "def hamming_distance(s1: str, s2: str) -> int:\n",
    "    \"\"\"Return the Hamming distance between equal-length sequences.\"\"\"\n",
    "    if len(s1) != len(s2):\n",
    "        raise ValueError(\"Undefined for sequences of unequal length.\")\n",
    "    return sum(el1 != el2 for el1, el2 in zip(s1, s2))\n",
    "\n",
    "def calculate_min_hamming_distance(samplesheet_path: str) -> list:\n",
    "    \"\"\"Calculate min Hamming distance for a run\"\"\"\n",
    "    try:\n",
    "        samplesheet_data = \\\n",
    "            get_samplesheet_records(samplesheets=[samplesheet_path,])\n",
    "        index_columns = ['index']\n",
    "        if 'index2' in samplesheet_data.columns:\n",
    "            samplesheet_data['index_length'] = \\\n",
    "                samplesheet_data.\\\n",
    "                apply(lambda x: len(x['index']) + len(x['index2']), axis=1)\n",
    "            index_columns.append('index2')\n",
    "        else:\n",
    "            samplesheet_data['index_length'] = \\\n",
    "                samplesheet_data.\\\n",
    "                apply(lambda x: len(x['index']), axis=1)\n",
    "        if 'Lane' in samplesheet_data.columns:\n",
    "            group_columns = [\n",
    "                'Lane', 'Sample_Project', 'index_length', 'Description']\n",
    "        else:\n",
    "            group_columns = [\n",
    "                'Sample_Project', 'index_length', 'Description']\n",
    "        output_rows = list()\n",
    "        for grp_name, g_data in samplesheet_data.groupby(group_columns):\n",
    "            min_hamming_dist = 10\n",
    "            index_data = \\\n",
    "                g_data[index_columns].to_dict(orient='records')\n",
    "            for i in range(0, len(index_data) - 1):\n",
    "                for j in range(i+1, len(index_data) - 1):\n",
    "                    if i != j:\n",
    "                        index_i = index_data[i].get('index')\n",
    "                        index_j = index_data[j].get('index')\n",
    "                        hamming_dist1 = \\\n",
    "                            hamming_distance(s1=index_i, s2=index_j)\n",
    "                        if min_hamming_dist > hamming_dist1:\n",
    "                            min_hamming_dist = hamming_dist1\n",
    "                        if 'index2' in index_columns:\n",
    "                            index2_i = index_data[i].get('index2')\n",
    "                            index2_j = index_data[j].get('index2')\n",
    "                            hamming_dist2 = \\\n",
    "                                hamming_distance(s1=index2_i, s2=index2_j)\n",
    "                            if min_hamming_dist > hamming_dist2:\n",
    "                                min_hamming_dist = hamming_dist2\n",
    "            group_row = dict(zip(group_columns, grp_name))\n",
    "            group_row.update({'min_hamming_distance': min_hamming_dist})\n",
    "            output_rows.append(group_row)\n",
    "        return output_rows\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cb199-185e-4db7-a225-68218e130ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_dir = '{{ REPORTS_DIR }}'\n",
    "run_dir = '{{ RUN_DIR }}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba099d7-a818-48b1-9733-a85a0a35328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(flowcell_summary_data_plot, flowcell_project_summary_plot,\n",
    " merged_df, flowcell_project_summary_table, sample_dist_plots,\n",
    " undetermined_plots, undetermined_table, combined_ihop_df,\n",
    " hop_plot) = \\\n",
    "    get_demult_report_and_plots_for_bclconvert(\n",
    "        reports_dir=reports_dir,\n",
    "        run_dir=run_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41daa76b-21aa-4a7d-90e0-132893a12700",
   "metadata": {},
   "source": [
    "## Flowcell total reads vs passing filter reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691cdae-47dc-4801-afeb-14e1abba753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(flowcell_summary_data_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d258091-6330-4a3a-9aa8-59c7ad2d1508",
   "metadata": {},
   "source": [
    "## Project summary plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaccf61-a0d5-416a-b05e-0cd1e610e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(flowcell_project_summary_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55476828-8963-468a-a974-3a66fb7fe914",
   "metadata": {},
   "source": [
    "## Project summary for lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a7f21-8ee8-40ee-844a-c79cd0ab2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(flowcell_project_summary_table.to_html(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37012721",
   "metadata": {},
   "source": [
    "## Hamming distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_rows = \\\n",
    "    calculate_min_hamming_distance(\n",
    "        samplesheet_path=os.path.join(reports_dir, 'SampleSheet.csv'))\n",
    "\n",
    "def style_low_hamming_distance(s: pd.Series, props: str = '', cut_off: int = 3) -> pd.Series:\n",
    "    return np.where(s < cut_off, props, '')\n",
    "\n",
    "html = \\\n",
    "    pd.DataFrame(output_rows).style.\\\n",
    "    apply(style_low_hamming_distance, props='color:red;', cut_off=3, axis=0, subset=['min_hamming_distance',]).\\\n",
    "    apply(style_low_hamming_distance, props='background-color:#ffffb3;', cut_off=3, axis=0, subset=['min_hamming_distance',]).\\\n",
    "    to_html(index=False)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01e9ff-6279-43bc-9aea-6f9ac4ee9001",
   "metadata": {},
   "source": [
    "## Sample read counts\n",
    "\n",
    "A list of samples with index barcodes and read count information can be found here. Please note that this table is hidden by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b35b9e-ccf7-43b5-8676-068d1310d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_low_read(s: pd.Series, props: str = '', cut_off: int = 500) -> pd.Series:\n",
    "    return np.where(s <= cut_off, props, '')\n",
    "\n",
    "html = merged_df.style.\\\n",
    "     apply(style_low_read, props='color:red;', cut_off=500, axis=0, subset=['# Reads',]).\\\n",
    "     apply(style_low_read, props='background-color:#ffffb3;', cut_off=500, axis=0, subset=['# Reads',]).\\\n",
    "     to_html(index=False)\n",
    "html = '<details><summary>Click to expand sample read count table</summary>' + html + '</details>'\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c80a0-d4d6-4546-9d2b-ecc1f07e5e9f",
   "metadata": {},
   "source": [
    "## Sample read count bar plot for lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd413f47-939d-4b02-a079-c86666cef452",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lane_id, p in sample_dist_plots.items():\n",
    "    print('Lane {0}'.format(lane_id))\n",
    "    display(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f024647-1b0b-45af-b16c-381e5d6535ca",
   "metadata": {},
   "source": [
    "## Undetermined reads\n",
    "\n",
    "A list of undetermined barcodes with read count information can be found here. This table is hidden by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866796d0-c480-4e5b-a900-59579e069307",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = undetermined_table.to_html(index=False)\n",
    "html = '<details><summary>Click to expand undetermined read count table</summary>' + html + '</details>'\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a39f1-d2f3-4625-bf0d-32515f28d933",
   "metadata": {},
   "source": [
    "## Undetermined read count bar plot for lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ab0c4-cb17-4160-86c7-3912dbb7c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lane_id, p in undetermined_plots.items():\n",
    "    print('Lane {0}'.format(lane_id))\n",
    "    display(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6ce23-56bd-4b2d-b213-dfed88d0affc",
   "metadata": {},
   "source": [
    "## Index hopping summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2bb8a8-1eef-4472-97a5-b4b20964ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(combined_ihop_df.to_html(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e8a46-e421-48a6-9c81-05e8fd78d327",
   "metadata": {},
   "source": [
    "## Index hopping bar plot for lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d5f5a-85b3-4ab3-89a6-1c21d64338ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abf3c4-d480-4874-a8ee-741f10d3d970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
